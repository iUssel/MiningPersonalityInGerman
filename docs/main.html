<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>main API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>main</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os

import helper
import miping

from pathlib import Path


def main():
    &#34;&#34;&#34;
    This function is called, when main.py is called as a script.
    It controls the whole data collection, preparation, and training process.
    &#34;&#34;&#34;

    # get configuration
    globalConfig, config_models, apiKeys = initialize()

    # initialize Twitter API with keys
    twitter = miping.interfaces.TwitterAPI(
        consumer_key=apiKeys[&#39;twitter&#39;][&#39;ConsumerKey&#39;],
        consumer_secret=apiKeys[&#39;twitter&#39;][&#39;ConsumerSecret&#39;],
        access_token=apiKeys[&#39;twitter&#39;][&#39;AccessToken&#39;],
        access_token_secret=apiKeys[&#39;twitter&#39;][&#39;AccessTokenSec&#39;],
        wait_on_rate_limit_notify=(
            globalConfig[&#34;twitter&#34;][&#34;wait_on_rate_limit_notify&#34;]
        ),

        additionalAttributes=globalConfig[&#34;twitter&#34;][&#34;add_attributes&#34;],
        removeNewLineChar=globalConfig[&#34;twitter&#34;][&#34;remove_new_line&#34;],
        ignoreRetweets=globalConfig[&#34;twitter&#34;][&#34;ignore_retweets&#34;],
    )

    # initialize result variables
    finalTweets = {}
    finalUsers = {}
    if globalConfig[&#34;process&#34;][&#34;scraping&#34;] is True:
        scrapeConf = globalConfig[&#34;scraping&#34;]
        # initialize maps interface
        maps = miping.interfaces.MapsAPI(
            apiKey=apiKeys[&#39;google&#39;][&#39;maps&#39;]
        )
        # initialize object
        scraping = helper.Scraping(
            config=globalConfig,
            twitter=twitter,
            maps=maps,
        )
        # get data from stream
        scrapedTweetsDict = scraping.doScrapingByLocation(
            readFiles=scrapeConf[&#34;scrapingByLoc&#34;][&#34;readFile&#34;],
            writeFiles=scrapeConf[&#34;scrapingByLoc&#34;][&#34;writeFile&#34;]
        )

        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:
            countryConf = globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;][country]
            # select users and some followers
            locationUsersCol, eligibleFolCol = scraping.doFollowerSelection(
                tweetSampleCol=scrapedTweetsDict[countryConf[&#39;name&#39;]],
                countryName=countryConf[&#39;name&#39;],
                readFiles=scrapeConf[&#34;followerSelect&#34;][&#34;readFile&#34;],
                writeFiles=scrapeConf[&#34;followerSelect&#34;][&#34;writeFile&#34;]
            )

            verifiedUsers, verifiedTweetCol = scraping.doUserSelection(
                country=country,
                locationUsersCol=locationUsersCol,
                eligibleFolCol=eligibleFolCol,
                readFiles=scrapeConf[&#34;userSelect&#34;][&#34;readFile&#34;],
                writeFiles=scrapeConf[&#34;userSelect&#34;][&#34;writeFile&#34;]
            )

            finalTweets[country] = verifiedTweetCol
            finalUsers[country] = verifiedUsers

    if globalConfig[&#34;process&#34;][&#34;dataPreparation&#34;] is True:
        preparationConfig = globalConfig[&#34;preparationProcess&#34;]

        # ibm api init
        ibmApi = miping.interfaces.IbmAPI(
            apiKey=apiKeys[&#39;ibm&#39;][&#39;api&#39;],
            url=apiKeys[&#39;ibm&#39;][&#39;url&#39;],
        )

        preparation = helper.PreparationProcess(
            config=globalConfig,
            ibm=ibmApi,
            twitter=twitter
        )

        # contains profile collections for each country
        # will be filled in next step
        globalProfileCollection = {}

        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:
            countryConf = globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;][country]
            if (
                preparationConfig[&#34;condenseTweets&#34;][&#34;readFile&#34;] is True or
                preparationConfig[&#34;hydrateUserID&#34;] is True
            ):
                # if we read from file, we do not need any input
                finalTweets[country] = None
                finalUsers[country] = None
            # prepare text and create empty profiles
            localProfileCollection = preparation.do_condense_tweets(
                verifiedTweetCol=finalTweets[country],
                verifiedUsers=finalUsers[country],
                language=countryConf[&#39;lang&#39;],
                country=country,
                readFiles=preparationConfig[&#34;condenseTweets&#34;][&#34;readFile&#34;],
                writeFiles=preparationConfig[&#34;condenseTweets&#34;][&#34;writeFile&#34;],
                hydrateUsers=preparationConfig[&#34;hydrateUserID&#34;]
            )

            # for desginated countries fill profiles with ibm data
            if country in preparationConfig[&#39;countriesIBM&#39;]:
                # get profiles for each user from IBM
                localProfileCollection = preparation.do_get_ibm_profiles(
                    profileCol=localProfileCollection,
                    country=country,
                    readFiles=preparationConfig[&#34;getIBMprofile&#34;][&#34;readFile&#34;],
                    writeFiles=preparationConfig[&#34;getIBMprofile&#34;][&#34;writeFile&#34;]
                )

            # global profile collection contains profiles inlcuding
            # ibm data, if available
            globalProfileCollection[country] = localProfileCollection

        # this is a manual step:
        # the extracted profiles will be enriched with LIWC
        # data. This is a separate program, therefore
        # we will ask the user if liwc files are provided
        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:
            # read profile collection from dict
            localProfileCollection = globalProfileCollection[country]

            # either read previously exported file or read LIWC output file
            localProfileCollection = preparation.do_liwc(
                    profileCol=localProfileCollection,
                    country=country,
                    liwcPath=preparationConfig[&#34;liwc&#34;][&#34;path&#34;],
                    fileName=preparationConfig[&#34;liwc&#34;][&#34;fileName&#34;],
                    readFiles=preparationConfig[&#34;liwc&#34;][&#34;readFile&#34;],
                    writeFiles=preparationConfig[&#34;liwc&#34;][&#34;writeFile&#34;],
                    skipInputWait=False
            )

            # global profile collection contains profiles inlcuding
            # ibm data, if available
            # now including liwc data
            globalProfileCollection[country] = localProfileCollection

        if preparationConfig[&#34;printStatistics&#34;] is True:
            preparation.print_statistics(globalProfileCollection)

    if globalConfig[&#34;process&#34;][&#34;modelTrainingLIWC&#34;] is True:
        trainConf = globalConfig[&#34;modelTraining&#34;]
        # init helper class
        trainingSteps = helper.TrainingProcess(
            config=trainConf,
            modelConfig=config_models
        )
        # build LIWC model based on English texts
        globalLIWCModels = trainingSteps.doLIWCModelTraining(
            profileCol=globalProfileCollection,
            writePickleFiles=trainConf[&#34;writePickleFiles&#34;],
            readPickleFiles=trainConf[&#34;readPickleFiles&#34;],
            writeONNXModel=trainConf[&#34;writeONNXModel&#34;],
            readONNXModel=trainConf[&#34;readONNXModel&#34;]
        )

    if globalConfig[&#34;process&#34;][&#34;derivePersonalities&#34;] is True:
        trainConf = globalConfig[&#34;modelTraining&#34;]
        # use trained model to derive German personalities
        # init helper class
        trainingSteps = helper.TrainingProcess(
            config=trainConf,
            modelConfig=config_models
        )
        # set variables to None, if we read files,
        # because they are not needed
        if trainConf[&#34;readFile&#34;] is True:
            globalLIWCModels = None
            locProfileCollection = None
            globalProfileCollection = {}
        else:
            locProfileCollection = globalProfileCollection[country]
        # loop over all available countries
        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:

            # for all countries we did not get IBM profiles for
            # we will fill profile with trained LIWC model
            filledProfileCollection = trainingSteps.predictPersonalitiesLIWC(
                profileCol=locProfileCollection,
                country=country,
                globalLIWCModels=globalLIWCModels,
                ibmList=globalConfig[&#34;preparationProcess&#34;][&#39;countriesIBM&#39;],
                readFiles=trainConf[&#34;readFile&#34;],
                writeFiles=trainConf[&#34;writeFile&#34;]
            )

            globalProfileCollection[country] = filledProfileCollection

        if globalConfig[&#34;preparationProcess&#34;][&#34;printStatistics&#34;] is True:
            # print again statistics
            preparation = helper.PreparationProcess(
                config=None,
                ibm=None
            )
            preparation.print_statistics(globalProfileCollection)

    if globalConfig[&#34;process&#34;][&#34;modelTrainingGloVe&#34;] is True:
        trainConf = globalConfig[&#34;modelTraining&#34;]
        # init helper class
        trainingSteps = helper.TrainingProcess(
            config=trainConf,
            modelConfig=config_models
        )
        # build GloVe model based on German texts
        globalGloVeModels = trainingSteps.doGloVeModelTraining(
            profileCol=globalProfileCollection[&#39;Germany&#39;],
            writePickleFiles=trainConf[&#34;writePickleFilesG&#34;],
            readPickleFiles=trainConf[&#34;readPickleFilesG&#34;],
            writeONNXModel=trainConf[&#34;writeONNXModelG&#34;],
            readONNXModel=trainConf[&#34;readONNXModelG&#34;],
            writeFeatureFile=trainConf[&#34;writeFeatureFile&#34;],
            readFeatureFile=trainConf[&#34;readFeatureFile&#34;],
        )

        # predict whole training set and print statistics
        # do prediction and print statistics
        trainingSteps.do_prediction(
            profileCol=globalProfileCollection[&#39;Germany&#39;],
            globalGloVeModels=globalGloVeModels,
            readFeatureFile=trainConf[&#34;readFeatureFile&#34;],
        )

    print(&#34;Finished&#34;)


def initialize():
    &#34;&#34;&#34;
    Initialize function reads configurations and environment variables

    Returns
    -------
    config : dict
        Global config, controlling the overall flow and parameters.
    config_models : dict
        Model configuration for grid search and tuning.
    apiKeys : dict
        Sensitive values stored locally in .env file, such as API keys.
    &#34;&#34;&#34;
    # load configuration
    configPath = Path(os.path.dirname(os.path.abspath(__file__)))
    configFullPath = configPath / &#34;config.yml&#34;
    configModelFullPath = configPath / &#34;config_models.yml&#34;
    configHelper = helper.ConfigLoader(
        configPath=configFullPath,
        modelConfigPath=configModelFullPath
    )
    config = configHelper.config
    config_models = configHelper.config_models

    # retrieve API keys and other secrets from environment variables
    apiKeys = configHelper.environmentVars

    return config, config_models, apiKeys


if __name__ == &#34;__main__&#34;:
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="main.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize function reads configurations and environment variables</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>Global config, controlling the overall flow and parameters.</dd>
<dt><strong><code>config_models</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model configuration for grid search and tuning.</dd>
<dt><strong><code>apiKeys</code></strong> :&ensp;<code>dict</code></dt>
<dd>Sensitive values stored locally in .env file, such as API keys.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize():
    &#34;&#34;&#34;
    Initialize function reads configurations and environment variables

    Returns
    -------
    config : dict
        Global config, controlling the overall flow and parameters.
    config_models : dict
        Model configuration for grid search and tuning.
    apiKeys : dict
        Sensitive values stored locally in .env file, such as API keys.
    &#34;&#34;&#34;
    # load configuration
    configPath = Path(os.path.dirname(os.path.abspath(__file__)))
    configFullPath = configPath / &#34;config.yml&#34;
    configModelFullPath = configPath / &#34;config_models.yml&#34;
    configHelper = helper.ConfigLoader(
        configPath=configFullPath,
        modelConfigPath=configModelFullPath
    )
    config = configHelper.config
    config_models = configHelper.config_models

    # retrieve API keys and other secrets from environment variables
    apiKeys = configHelper.environmentVars

    return config, config_models, apiKeys</code></pre>
</details>
</dd>
<dt id="main.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is called, when main.py is called as a script.
It controls the whole data collection, preparation, and training process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    &#34;&#34;&#34;
    This function is called, when main.py is called as a script.
    It controls the whole data collection, preparation, and training process.
    &#34;&#34;&#34;

    # get configuration
    globalConfig, config_models, apiKeys = initialize()

    # initialize Twitter API with keys
    twitter = miping.interfaces.TwitterAPI(
        consumer_key=apiKeys[&#39;twitter&#39;][&#39;ConsumerKey&#39;],
        consumer_secret=apiKeys[&#39;twitter&#39;][&#39;ConsumerSecret&#39;],
        access_token=apiKeys[&#39;twitter&#39;][&#39;AccessToken&#39;],
        access_token_secret=apiKeys[&#39;twitter&#39;][&#39;AccessTokenSec&#39;],
        wait_on_rate_limit_notify=(
            globalConfig[&#34;twitter&#34;][&#34;wait_on_rate_limit_notify&#34;]
        ),

        additionalAttributes=globalConfig[&#34;twitter&#34;][&#34;add_attributes&#34;],
        removeNewLineChar=globalConfig[&#34;twitter&#34;][&#34;remove_new_line&#34;],
        ignoreRetweets=globalConfig[&#34;twitter&#34;][&#34;ignore_retweets&#34;],
    )

    # initialize result variables
    finalTweets = {}
    finalUsers = {}
    if globalConfig[&#34;process&#34;][&#34;scraping&#34;] is True:
        scrapeConf = globalConfig[&#34;scraping&#34;]
        # initialize maps interface
        maps = miping.interfaces.MapsAPI(
            apiKey=apiKeys[&#39;google&#39;][&#39;maps&#39;]
        )
        # initialize object
        scraping = helper.Scraping(
            config=globalConfig,
            twitter=twitter,
            maps=maps,
        )
        # get data from stream
        scrapedTweetsDict = scraping.doScrapingByLocation(
            readFiles=scrapeConf[&#34;scrapingByLoc&#34;][&#34;readFile&#34;],
            writeFiles=scrapeConf[&#34;scrapingByLoc&#34;][&#34;writeFile&#34;]
        )

        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:
            countryConf = globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;][country]
            # select users and some followers
            locationUsersCol, eligibleFolCol = scraping.doFollowerSelection(
                tweetSampleCol=scrapedTweetsDict[countryConf[&#39;name&#39;]],
                countryName=countryConf[&#39;name&#39;],
                readFiles=scrapeConf[&#34;followerSelect&#34;][&#34;readFile&#34;],
                writeFiles=scrapeConf[&#34;followerSelect&#34;][&#34;writeFile&#34;]
            )

            verifiedUsers, verifiedTweetCol = scraping.doUserSelection(
                country=country,
                locationUsersCol=locationUsersCol,
                eligibleFolCol=eligibleFolCol,
                readFiles=scrapeConf[&#34;userSelect&#34;][&#34;readFile&#34;],
                writeFiles=scrapeConf[&#34;userSelect&#34;][&#34;writeFile&#34;]
            )

            finalTweets[country] = verifiedTweetCol
            finalUsers[country] = verifiedUsers

    if globalConfig[&#34;process&#34;][&#34;dataPreparation&#34;] is True:
        preparationConfig = globalConfig[&#34;preparationProcess&#34;]

        # ibm api init
        ibmApi = miping.interfaces.IbmAPI(
            apiKey=apiKeys[&#39;ibm&#39;][&#39;api&#39;],
            url=apiKeys[&#39;ibm&#39;][&#39;url&#39;],
        )

        preparation = helper.PreparationProcess(
            config=globalConfig,
            ibm=ibmApi,
            twitter=twitter
        )

        # contains profile collections for each country
        # will be filled in next step
        globalProfileCollection = {}

        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:
            countryConf = globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;][country]
            if (
                preparationConfig[&#34;condenseTweets&#34;][&#34;readFile&#34;] is True or
                preparationConfig[&#34;hydrateUserID&#34;] is True
            ):
                # if we read from file, we do not need any input
                finalTweets[country] = None
                finalUsers[country] = None
            # prepare text and create empty profiles
            localProfileCollection = preparation.do_condense_tweets(
                verifiedTweetCol=finalTweets[country],
                verifiedUsers=finalUsers[country],
                language=countryConf[&#39;lang&#39;],
                country=country,
                readFiles=preparationConfig[&#34;condenseTweets&#34;][&#34;readFile&#34;],
                writeFiles=preparationConfig[&#34;condenseTweets&#34;][&#34;writeFile&#34;],
                hydrateUsers=preparationConfig[&#34;hydrateUserID&#34;]
            )

            # for desginated countries fill profiles with ibm data
            if country in preparationConfig[&#39;countriesIBM&#39;]:
                # get profiles for each user from IBM
                localProfileCollection = preparation.do_get_ibm_profiles(
                    profileCol=localProfileCollection,
                    country=country,
                    readFiles=preparationConfig[&#34;getIBMprofile&#34;][&#34;readFile&#34;],
                    writeFiles=preparationConfig[&#34;getIBMprofile&#34;][&#34;writeFile&#34;]
                )

            # global profile collection contains profiles inlcuding
            # ibm data, if available
            globalProfileCollection[country] = localProfileCollection

        # this is a manual step:
        # the extracted profiles will be enriched with LIWC
        # data. This is a separate program, therefore
        # we will ask the user if liwc files are provided
        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:
            # read profile collection from dict
            localProfileCollection = globalProfileCollection[country]

            # either read previously exported file or read LIWC output file
            localProfileCollection = preparation.do_liwc(
                    profileCol=localProfileCollection,
                    country=country,
                    liwcPath=preparationConfig[&#34;liwc&#34;][&#34;path&#34;],
                    fileName=preparationConfig[&#34;liwc&#34;][&#34;fileName&#34;],
                    readFiles=preparationConfig[&#34;liwc&#34;][&#34;readFile&#34;],
                    writeFiles=preparationConfig[&#34;liwc&#34;][&#34;writeFile&#34;],
                    skipInputWait=False
            )

            # global profile collection contains profiles inlcuding
            # ibm data, if available
            # now including liwc data
            globalProfileCollection[country] = localProfileCollection

        if preparationConfig[&#34;printStatistics&#34;] is True:
            preparation.print_statistics(globalProfileCollection)

    if globalConfig[&#34;process&#34;][&#34;modelTrainingLIWC&#34;] is True:
        trainConf = globalConfig[&#34;modelTraining&#34;]
        # init helper class
        trainingSteps = helper.TrainingProcess(
            config=trainConf,
            modelConfig=config_models
        )
        # build LIWC model based on English texts
        globalLIWCModels = trainingSteps.doLIWCModelTraining(
            profileCol=globalProfileCollection,
            writePickleFiles=trainConf[&#34;writePickleFiles&#34;],
            readPickleFiles=trainConf[&#34;readPickleFiles&#34;],
            writeONNXModel=trainConf[&#34;writeONNXModel&#34;],
            readONNXModel=trainConf[&#34;readONNXModel&#34;]
        )

    if globalConfig[&#34;process&#34;][&#34;derivePersonalities&#34;] is True:
        trainConf = globalConfig[&#34;modelTraining&#34;]
        # use trained model to derive German personalities
        # init helper class
        trainingSteps = helper.TrainingProcess(
            config=trainConf,
            modelConfig=config_models
        )
        # set variables to None, if we read files,
        # because they are not needed
        if trainConf[&#34;readFile&#34;] is True:
            globalLIWCModels = None
            locProfileCollection = None
            globalProfileCollection = {}
        else:
            locProfileCollection = globalProfileCollection[country]
        # loop over all available countries
        for country in globalConfig[&#39;twitter&#39;][&#39;coordinates&#39;]:

            # for all countries we did not get IBM profiles for
            # we will fill profile with trained LIWC model
            filledProfileCollection = trainingSteps.predictPersonalitiesLIWC(
                profileCol=locProfileCollection,
                country=country,
                globalLIWCModels=globalLIWCModels,
                ibmList=globalConfig[&#34;preparationProcess&#34;][&#39;countriesIBM&#39;],
                readFiles=trainConf[&#34;readFile&#34;],
                writeFiles=trainConf[&#34;writeFile&#34;]
            )

            globalProfileCollection[country] = filledProfileCollection

        if globalConfig[&#34;preparationProcess&#34;][&#34;printStatistics&#34;] is True:
            # print again statistics
            preparation = helper.PreparationProcess(
                config=None,
                ibm=None
            )
            preparation.print_statistics(globalProfileCollection)

    if globalConfig[&#34;process&#34;][&#34;modelTrainingGloVe&#34;] is True:
        trainConf = globalConfig[&#34;modelTraining&#34;]
        # init helper class
        trainingSteps = helper.TrainingProcess(
            config=trainConf,
            modelConfig=config_models
        )
        # build GloVe model based on German texts
        globalGloVeModels = trainingSteps.doGloVeModelTraining(
            profileCol=globalProfileCollection[&#39;Germany&#39;],
            writePickleFiles=trainConf[&#34;writePickleFilesG&#34;],
            readPickleFiles=trainConf[&#34;readPickleFilesG&#34;],
            writeONNXModel=trainConf[&#34;writeONNXModelG&#34;],
            readONNXModel=trainConf[&#34;readONNXModelG&#34;],
            writeFeatureFile=trainConf[&#34;writeFeatureFile&#34;],
            readFeatureFile=trainConf[&#34;readFeatureFile&#34;],
        )

        # predict whole training set and print statistics
        # do prediction and print statistics
        trainingSteps.do_prediction(
            profileCol=globalProfileCollection[&#39;Germany&#39;],
            globalGloVeModels=globalGloVeModels,
            readFeatureFile=trainConf[&#34;readFeatureFile&#34;],
        )

    print(&#34;Finished&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="main.initialize" href="#main.initialize">initialize</a></code></li>
<li><code><a title="main.main" href="#main.main">main</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>