<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>helper.preparationProcess API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>helper.preparationProcess</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy

from pathlib import Path
from miping.models.profile import Profile
from miping.models.profileCollection import ProfileCollection
from miping.training.dataPreparation import DataPreparation
from miping.interfaces.helper import Helper
from miping.interfaces.liwc import LiwcAPI
from miping.models import TweetCollection


class PreparationProcess:
    &#34;&#34;&#34;
    Wrapper class for data preparation process (2nd step).
    Contains all functions needed for preparation. Calls mostly miping
    module functions and allows imports and exports of data via csv.
    &#34;&#34;&#34;

    def __init__(
        self,
        config,
        ibm=None,
        twitter=None,
    ):
        &#34;&#34;&#34;
        Init function to save configuration and API objects.

        Parameters
        ----------
        config : dict, default=None, required
            Configuration object as returned from ConfigLoader class.
        ibm : miping.interfaces.IbmAPI, default=None
            Initialized IbmAPI object, ready for calls.
        twitter : miping.interfaces.TwitterAPI, default=None
            Initialized TwitterAPI object, ready for calls.
        &#34;&#34;&#34;
        # save config
        self.config = config

        # save ibm object
        self.ibm = ibm

        # save twitter api object
        self.twitter = twitter

        return

    def hydrate_users(
        self,
        country,
    ):
        &#34;&#34;&#34;
        Fetch and return user profiles and tweet list for user ID list
        in CSV file.

        Twitter allows only to pass tweet and user IDs, when publicly sharing
        scraped data. This function allows to import a CSV file containing
        only user IDs and automatically fetching related user objects and
        for each user up to the latest 250 tweets. Private users are skipped.
        The CSV file path is hard coded.
        Expected CSV path: &#39;data/04&#39; + country + &#39;UserIDs.csv&#39;

        Parameters
        ----------
        country : string, default=None, required
            The country parameter is used to differentiate between CSV files.
            The country name is automatically added to the file name. It
            should match a country given in the global config.

        Returns
        -------
        tweetCol : miping.models.TweetCollection
            Data model type in miping module. Contains the retrieved tweets
            for all given users.
        usersCol : miping.models.UserCollection
            Data model type in miping module. Contains user objects as
            returned by Twitter API.
        &#34;&#34;&#34;
        # load user ids from file
        # path for saved profiles
        file_directory_string = (
            &#39;data/04&#39; +
            country +
            &#39;UserIDs.csv&#39;
        )
        file_path = Path(file_directory_string)

        # read in user ids
        profileCol = ProfileCollection()
        profileCol.read_profile_list_file(
            full_path=file_path,
            idsonly=True
        )

        # extract userIDs as list
        idList = [obj.userID for obj in profileCol.profileList]
        # create user object
        usersCol = self.twitter.getUsersByList(idList)

        # create tweet collection for return
        tweetCol = TweetCollection(
            additionalAttributes=self.config[&#34;twitter&#34;][&#34;add_attributes&#34;]
        )
        for user in usersCol.userList:
            # get tweets for given user
            singleTweetCol = self.twitter.funcGetTweetListByUser(
                    userID=user.id_str,
                    limit=250,
            )
            # add to total collection
            tweetCol.add_tweet_collection(singleTweetCol)

        print(&#34;Hydration finished&#34;)

        return tweetCol, usersCol

    def do_condense_tweets(
        self,
        verifiedTweetCol,
        verifiedUsers,
        language,
        country,
        readFiles=False,
        writeFiles=False,
        hydrateUsers=False,
    ):
        &#34;&#34;&#34;
        Actual data preparation for each user. Combining tweets to string.

        This function allows to import and export results via CSV. When
        importing the actual program code is skipped.
        Expected CSV path: &#39;data/04condensed&#39; + country + &#39;profiles.csv&#39;.
        It is possible to start this process with just a list of
        user ids, then hydrate_users() has to be called first.
        For each user in the list all tweets are combined into a single
        string. Then the cleaning function is called to remove
        unwanted characters. Only if the resulting string has 600 or
        more space separated tokens, a Profile object is created for that
        user.

        Parameters
        ----------
        verifiedTweetCol : miping.models.TweetCollection, default=None, req
            Previously collected tweets as tweet collection. Contains all
            tweets that should be condensed and combined with the
            user objects.
        verifiedUsers : miping.models.UserCollection, default=None, req
            Previously collected users as user collection. These will be
            combined with the tweet collection.
        language : string, default=None, required
            Language (two letter ISO code) for given contry
            (as specified in config).
        country : string, default=None, required
            Country name of where the passed users are collected from
            (as specified in config)
        readFiles : boolean, default=False
            If True, CSV files will be read instead of following program
            logic.
        writeFiles : boolean, default=False
            Can only be True, if readFiles is False. If True, will export
            results to CSV files. Allows to read files in the next program
            run.
        hydrateUsers : boolean, default=False
            Only True, if readFiles is False. Will call hydrate_users() and
            and take its results as input for condensing. In this case
            verifiedTweetCol and verifiedUsers can be passed as empty
            objects.

        Returns
        -------
        returnProfileCol : miping.models.ProfileCollection
            Profile collection containing all profiles created.
            Profiles include tweets and users.
        &#34;&#34;&#34;

        if writeFiles is True and readFiles is True:
            raise Exception(
                &#34;readFiles and writeFiles cannot be True at the same time.&#34;
            )

        dataPre = DataPreparation(
        )

        returnProfileCol = ProfileCollection()

        if readFiles is True:
            print(&#34;\nReading files for condense tweets&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )

            # path for saved profiles
            file_directory_string = (
                &#39;data/04condensed&#39; +
                country +
                &#39;profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.read_profile_list_file(
                full_path=file_path
            )

            print(&#34;Files successfully loaded&#34;)
        else:
            print(&#34;\nBegin condensing tweets&#34;)
            print(&#34;Country: &#34; + str(country))

            if hydrateUsers is True:
                print(&#34;Hydrating user ids from file&#34;)
                verifiedTweetCol, verifiedUsers = self.hydrate_users(
                    country=country
                )

            # iterate over all users to condense their tweets
            # and create a profile to add to profileCollection
            for user in verifiedUsers.userList:
                userID = user.id_str
                # get all saved tweets for this user
                userTweetCol = verifiedTweetCol.get_tweets_of_userid(
                    userID=userID
                )
                # save number of tweets in list
                tweetCount = len(userTweetCol.tweetList)
                # combine all tweets into one string
                textString = userTweetCol.combine_tweet_text()
                # call data cleansing for combined string
                textString = dataPre.clean_text(
                    textString=textString
                )

                # count words (based on any whitespace)
                wordCount = len(textString.split())

                # IBM says they need at least 600 words
                # for analysis, only then we are adding to collection
                if wordCount &gt;= 600:

                    singleProfile = Profile(
                        userID=userID,
                        text=textString,
                        numberWords=wordCount,
                        numberTweets=tweetCount,
                        language=language
                    )

                    # add profile to collection
                    returnProfileCol.add_profile(singleProfile)

            # only write file if specified
            if writeFiles is True:
                # path for saving profileCollection
                file_directory_string = (
                    &#39;data/04condensed&#39; +
                    country +
                    &#39;profiles.csv&#39;
                )
                file_path = Path(file_directory_string)

                returnProfileCol.write_profile_list_file(
                    full_path=file_path
                )
            print(&#34;End condensing tweets&#34;)

        return returnProfileCol

    def do_get_ibm_profiles(
        self,
        profileCol,
        country,
        readFiles=False,
        writeFiles=False,
    ):
        &#34;&#34;&#34;
        Query IBM API for given ProfileCollection for personality scores.

        Allows imports and exports of results via CSV. Expected path is
        &#39;data/05&#39; + country + &#39;ibm_profiles.csv&#39;. For each profile
        in given profile collection, IBM API is called and the existing
        profile enriched by IBM personality information. Result
        is returned as enriched profile collection. If errors occur for
        a user (e.g. too few words), this user will be excluded.
        A progress bar is shown during this process. Note that IBM
        does not support all languages.

        Parameters
        ----------
        profileCol : miping.models.ProfileCollection, default=None, required
            Profile collection that should be enriched.
        country : string, default=None, required
            Country name of where the passed users are collected from
            (as specified in config)
        readFiles : boolean, default=False
            If True, CSV files will be read instead of following program
            logic.
        writeFiles : boolean, default=False
            Can only be True, if readFiles is False. If True, will export
            results to CSV files. Allows to read files in the next program
            run.

        Returns
        -------
        returnProfileCol : ProfileCollection
            New IBM enriched ProfileCollection based on the input collection.
        &#34;&#34;&#34;
        if writeFiles is True and readFiles is True:
            raise Exception(
                &#34;readFiles and writeFiles cannot be True at the same time.&#34;
            )

        returnProfileCol = ProfileCollection()

        if readFiles is True:
            print(&#34;\nReading files for get ibm profiles&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )

            # path for saved profiles
            file_directory_string = (
                &#39;data/05&#39; +
                country +
                &#39;ibm_profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.read_profile_list_file(
                full_path=file_path
            )

            print(&#34;Files successfully loaded&#34;)
        else:
            print(&#34;\nBegin getting IBM profiles&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )
            helper = Helper()

            # initialize progress bar
            numProfiles = len(profileCol.profileList)
            helper.printProgressBar(
                0,
                numProfiles,
                prefix=&#39;Progress:&#39;,
                suffix=&#39;Complete&#39;,
                length=50
            )

            # iterate over all profiles, call api
            # and enrich profiles with result
            for num, profile in enumerate(profileCol.profileList):
                profile, errorEncounterd = self.ibm.get_profile(
                    text=profile.text,
                    fillProfile=profile,
                    language=profile.language,
                )
                if errorEncounterd is False:
                    # add profile to collection
                    returnProfileCol.add_profile(profile)

                # Update Progress Bar
                helper.printProgressBar(
                    num + 1,
                    numProfiles,
                    prefix=&#39;Progress:&#39;,
                    suffix=&#39;Complete&#39;,
                    length=50
                )

            # only write file if specified
            if writeFiles is True:
                # path for saving profileCollection
                file_directory_string = (
                    &#39;data/05&#39; +
                    country +
                    &#39;ibm_profiles.csv&#39;
                )
                file_path = Path(file_directory_string)

                returnProfileCol.write_profile_list_file(
                    full_path=file_path
                )
            print(&#34;End getting IBM profiles&#34;)

        return returnProfileCol

    def do_liwc(
        self,
        profileCol,
        country,
        liwcPath,
        fileName,
        readFiles=False,
        writeFiles=False,
        skipInputWait=False
    ):
        &#34;&#34;&#34;
        Return enriched profiles with LIWC data from separate file.

        Allows imports and exports of results via CSV. Expected path is
        &#39;data/06&#39; + country + &#39;liwc_profiles.csv&#39;.
        The input profile collection contains tweet and user data and
        should be enriched with LIWC data. Since LIWC is a standalone
        program it cannot be integrated into the Python program.
        The user is asked to manually prepare the previously exported
        profile collection with LIWC and then export the LIWC results.
        These results are saved in CSV format in liwcPath and fileName.
        From there they are imported via LIWC API and enrich the existing
        profiles.

        Parameters
        ----------
        profileCol : ProfileCollection, default=None, required
            Input profile collection which should be enriched with LIWC data.
        country : string, default=None, required
            Country name of where the passed users are collected from
            (as specified in config)
        liwcPath : string, default=None, required
            Relative path of where the LIWC export is saved.
        fileName : string, default=None, required
            File name for LIWC export.
        readFiles : boolean, default=False
            If True, CSV files will be read instead of following program
            logic.
        writeFiles : boolean, default=False
            Can only be True, if readFiles is False. If True, will export
            results to CSV files. Allows to read files in the next program
            run.
        skipInputWait : boolean, default=False
            If False, the user needs to manually confirm by pressing enter
            that the LIWC export is ready for import. If True, this
            confirmation step is skipped and the program assumes the
            file already exists.

        Returns
        -------
        returnProfileCol : ProfileCollection
            New LIWC enriched ProfileCollection based on the input collection.
        &#34;&#34;&#34;

        if writeFiles is True and readFiles is True:
            raise Exception(
                &#34;readFiles and writeFiles cannot be True at the same time.&#34;
            )

        returnProfileCol = ProfileCollection()

        if readFiles is True:
            print(&#34;\nReading files for previous liwc exports&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )

            # path for saved profiles
            file_directory_string = (
                &#39;data/06&#39; +
                country +
                &#39;liwc_profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.read_profile_list_file(
                full_path=file_path
            )

            print(&#34;Files successfully loaded&#34;)
        else:
            # path where LIWC results where saved
            file_directory_string = (
                liwcPath +
                country +
                fileName
            )
            file_path = Path(file_directory_string)

            print(
                &#34;\nBegin LIWC loading. This is a manual task. &#34; +
                &#34;Please ensure that LIWC results exist in &#34; +
                &#34;the following location: &#34; +
                str(file_directory_string)
            )
            filesReady = True
            if skipInputWait is False:
                # waiting for any user input
                # so user can create LIWC output files
                val = input(&#34;Please confirm with enter when files are ready: &#34;)
                if len(val) &gt; 0:
                    # user provided some input other than enter
                    filesReady = False
                    print(&#34;Skipping process&#34;)

            if filesReady is True:
                # import liwc results and add to profiles
                liwc = LiwcAPI()
                returnProfileCol = liwc.import_liwc_result(
                    fullPath=file_path,
                    profileCol=profileCol
                )

            # only write file if specified
            if writeFiles is True:
                # path for saving profileCollection
                file_directory_string = (
                    &#39;data/06&#39; +
                    country +
                    &#39;liwc_profiles.csv&#39;
                )
                file_path = Path(file_directory_string)

                returnProfileCol.write_profile_list_file(
                    full_path=file_path
                )
            print(&#34;End LIWC loading.&#34;)

        return returnProfileCol

    def print_statistics(
        self,
        globalProfileCollection,
    ):
        &#34;&#34;&#34;
        Print descriptive statistics for given profile collections.

        The given dictionary contains one profile collection for
        each country. Information will be printed about the number
        of users, number of words, number of tweets, and the Big
        Five personality scores.
        In the end and additional check is carried out to see if any
        user has more than 250 tweets in the collection. Since we
        selected only 250 tweets for each user, more tweets would
        indicate that we accidentally selected this user twice.

        Parameters
        ----------
        globalProfileCollection : dict, default=None, required
            Dictionary containing profile collections for each country.
            For each collection the descriptive statistics are printed.
        &#34;&#34;&#34;

        print(&#34;\nData Preparation Statistics&#34;)
        for idx, country in enumerate(globalProfileCollection):
            print(&#34;Statistics for: &#34; + str(country))

            locProfileList = globalProfileCollection[country].profileList

            # number of users
            numUsers = len(locProfileList)
            print(&#34;Number of users: &#34; + str(numUsers))

            # number of words per user
            print(&#34;Number of words&#34;)
            numWords = [
                float(getattr(profile, &#39;numberWords&#39;))
                for profile in locProfileList
            ]
            self.print_min_max_mean_std(numWords)
            # number of tweets
            print(&#34;Number of tweets&#34;)
            numTweets = [
                float(getattr(profile, &#39;numberTweets&#39;))
                for profile in locProfileList
            ]
            self.print_min_max_mean_std(numTweets)

            # ibm big 5 data
            locAttrList = [
                &#39;big5_openness&#39;,
                &#39;big5_conscientiousness&#39;,
                &#39;big5_extraversion&#39;,
                &#39;big5_agreeableness&#39;,
                &#39;big5_neuroticism&#39;
            ]

            for attr in locAttrList:
                print(attr)
                seq = []
                for profile in locProfileList:
                    value = getattr(profile, attr)
                    if value == &#39;&#39;:
                        # append nothing if no value given
                        continue
                    seq.append(float(value))
                self.print_min_max_mean_std(seq)

            # duplicate check (there should be no more than 250 tweets)
            print(&#34;User with more than 250 tweets (check for duplicates)&#34;)
            for profile in locProfileList:
                if int(getattr(profile, &#39;numberTweets&#39;)) &gt; 250:
                    print(profile)
                    print(profile.userID)
            print(
                &#34;END users with more than 250 tweets &#34; +
                &#34;(if no users shown it&#39;s fine)&#34;
            )

        print(&#34;Finished\n&#34;)
        return

    def print_min_max_mean_std(
        self,
        printList,
        percentile=False
    ):
        &#34;&#34;&#34;
        Print min, max, mean, std, and 25th and 75th percentile for values.

        Parameters
        ----------
        printList : list, default=None, required
            List containing numeric values for which descriptive statistics
            should be calculated and printed.
        percentile : boolean, default=False
            If True, additionally 25th and 75th percentile are printed.
        &#34;&#34;&#34;
        if len(printList) &gt; 0:
            print(&#34;MIN: &#34; + str(min(printList)))
            if percentile is True:
                print(&#34;25th percentile: &#34;, numpy.percentile(printList, 25))
                print(&#34;75th percentile: &#34;, numpy.percentile(printList, 75))
            print(&#34;MAX: &#34; + str(max(printList)))
            print(&#34;Mean: &#34; + str(numpy.mean(printList)))
            print(&#34;Standard Deviation: &#34; + str(numpy.std(printList)))
        else:
            print(&#39;No value in list&#39;)

        return</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="helper.preparationProcess.PreparationProcess"><code class="flex name class">
<span>class <span class="ident">PreparationProcess</span></span>
<span>(</span><span>config, ibm=None, twitter=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper class for data preparation process (2nd step).
Contains all functions needed for preparation. Calls mostly miping
module functions and allows imports and exports of data via csv.</p>
<p>Init function to save configuration and API objects.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code>, default=<code>None, required</code></dt>
<dd>Configuration object as returned from ConfigLoader class.</dd>
<dt><strong><code>ibm</code></strong> :&ensp;<code>miping.interfaces.IbmAPI</code>, default=<code>None</code></dt>
<dd>Initialized IbmAPI object, ready for calls.</dd>
<dt><strong><code>twitter</code></strong> :&ensp;<code>miping.interfaces.TwitterAPI</code>, default=<code>None</code></dt>
<dd>Initialized TwitterAPI object, ready for calls.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PreparationProcess:
    &#34;&#34;&#34;
    Wrapper class for data preparation process (2nd step).
    Contains all functions needed for preparation. Calls mostly miping
    module functions and allows imports and exports of data via csv.
    &#34;&#34;&#34;

    def __init__(
        self,
        config,
        ibm=None,
        twitter=None,
    ):
        &#34;&#34;&#34;
        Init function to save configuration and API objects.

        Parameters
        ----------
        config : dict, default=None, required
            Configuration object as returned from ConfigLoader class.
        ibm : miping.interfaces.IbmAPI, default=None
            Initialized IbmAPI object, ready for calls.
        twitter : miping.interfaces.TwitterAPI, default=None
            Initialized TwitterAPI object, ready for calls.
        &#34;&#34;&#34;
        # save config
        self.config = config

        # save ibm object
        self.ibm = ibm

        # save twitter api object
        self.twitter = twitter

        return

    def hydrate_users(
        self,
        country,
    ):
        &#34;&#34;&#34;
        Fetch and return user profiles and tweet list for user ID list
        in CSV file.

        Twitter allows only to pass tweet and user IDs, when publicly sharing
        scraped data. This function allows to import a CSV file containing
        only user IDs and automatically fetching related user objects and
        for each user up to the latest 250 tweets. Private users are skipped.
        The CSV file path is hard coded.
        Expected CSV path: &#39;data/04&#39; + country + &#39;UserIDs.csv&#39;

        Parameters
        ----------
        country : string, default=None, required
            The country parameter is used to differentiate between CSV files.
            The country name is automatically added to the file name. It
            should match a country given in the global config.

        Returns
        -------
        tweetCol : miping.models.TweetCollection
            Data model type in miping module. Contains the retrieved tweets
            for all given users.
        usersCol : miping.models.UserCollection
            Data model type in miping module. Contains user objects as
            returned by Twitter API.
        &#34;&#34;&#34;
        # load user ids from file
        # path for saved profiles
        file_directory_string = (
            &#39;data/04&#39; +
            country +
            &#39;UserIDs.csv&#39;
        )
        file_path = Path(file_directory_string)

        # read in user ids
        profileCol = ProfileCollection()
        profileCol.read_profile_list_file(
            full_path=file_path,
            idsonly=True
        )

        # extract userIDs as list
        idList = [obj.userID for obj in profileCol.profileList]
        # create user object
        usersCol = self.twitter.getUsersByList(idList)

        # create tweet collection for return
        tweetCol = TweetCollection(
            additionalAttributes=self.config[&#34;twitter&#34;][&#34;add_attributes&#34;]
        )
        for user in usersCol.userList:
            # get tweets for given user
            singleTweetCol = self.twitter.funcGetTweetListByUser(
                    userID=user.id_str,
                    limit=250,
            )
            # add to total collection
            tweetCol.add_tweet_collection(singleTweetCol)

        print(&#34;Hydration finished&#34;)

        return tweetCol, usersCol

    def do_condense_tweets(
        self,
        verifiedTweetCol,
        verifiedUsers,
        language,
        country,
        readFiles=False,
        writeFiles=False,
        hydrateUsers=False,
    ):
        &#34;&#34;&#34;
        Actual data preparation for each user. Combining tweets to string.

        This function allows to import and export results via CSV. When
        importing the actual program code is skipped.
        Expected CSV path: &#39;data/04condensed&#39; + country + &#39;profiles.csv&#39;.
        It is possible to start this process with just a list of
        user ids, then hydrate_users() has to be called first.
        For each user in the list all tweets are combined into a single
        string. Then the cleaning function is called to remove
        unwanted characters. Only if the resulting string has 600 or
        more space separated tokens, a Profile object is created for that
        user.

        Parameters
        ----------
        verifiedTweetCol : miping.models.TweetCollection, default=None, req
            Previously collected tweets as tweet collection. Contains all
            tweets that should be condensed and combined with the
            user objects.
        verifiedUsers : miping.models.UserCollection, default=None, req
            Previously collected users as user collection. These will be
            combined with the tweet collection.
        language : string, default=None, required
            Language (two letter ISO code) for given contry
            (as specified in config).
        country : string, default=None, required
            Country name of where the passed users are collected from
            (as specified in config)
        readFiles : boolean, default=False
            If True, CSV files will be read instead of following program
            logic.
        writeFiles : boolean, default=False
            Can only be True, if readFiles is False. If True, will export
            results to CSV files. Allows to read files in the next program
            run.
        hydrateUsers : boolean, default=False
            Only True, if readFiles is False. Will call hydrate_users() and
            and take its results as input for condensing. In this case
            verifiedTweetCol and verifiedUsers can be passed as empty
            objects.

        Returns
        -------
        returnProfileCol : miping.models.ProfileCollection
            Profile collection containing all profiles created.
            Profiles include tweets and users.
        &#34;&#34;&#34;

        if writeFiles is True and readFiles is True:
            raise Exception(
                &#34;readFiles and writeFiles cannot be True at the same time.&#34;
            )

        dataPre = DataPreparation(
        )

        returnProfileCol = ProfileCollection()

        if readFiles is True:
            print(&#34;\nReading files for condense tweets&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )

            # path for saved profiles
            file_directory_string = (
                &#39;data/04condensed&#39; +
                country +
                &#39;profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.read_profile_list_file(
                full_path=file_path
            )

            print(&#34;Files successfully loaded&#34;)
        else:
            print(&#34;\nBegin condensing tweets&#34;)
            print(&#34;Country: &#34; + str(country))

            if hydrateUsers is True:
                print(&#34;Hydrating user ids from file&#34;)
                verifiedTweetCol, verifiedUsers = self.hydrate_users(
                    country=country
                )

            # iterate over all users to condense their tweets
            # and create a profile to add to profileCollection
            for user in verifiedUsers.userList:
                userID = user.id_str
                # get all saved tweets for this user
                userTweetCol = verifiedTweetCol.get_tweets_of_userid(
                    userID=userID
                )
                # save number of tweets in list
                tweetCount = len(userTweetCol.tweetList)
                # combine all tweets into one string
                textString = userTweetCol.combine_tweet_text()
                # call data cleansing for combined string
                textString = dataPre.clean_text(
                    textString=textString
                )

                # count words (based on any whitespace)
                wordCount = len(textString.split())

                # IBM says they need at least 600 words
                # for analysis, only then we are adding to collection
                if wordCount &gt;= 600:

                    singleProfile = Profile(
                        userID=userID,
                        text=textString,
                        numberWords=wordCount,
                        numberTweets=tweetCount,
                        language=language
                    )

                    # add profile to collection
                    returnProfileCol.add_profile(singleProfile)

            # only write file if specified
            if writeFiles is True:
                # path for saving profileCollection
                file_directory_string = (
                    &#39;data/04condensed&#39; +
                    country +
                    &#39;profiles.csv&#39;
                )
                file_path = Path(file_directory_string)

                returnProfileCol.write_profile_list_file(
                    full_path=file_path
                )
            print(&#34;End condensing tweets&#34;)

        return returnProfileCol

    def do_get_ibm_profiles(
        self,
        profileCol,
        country,
        readFiles=False,
        writeFiles=False,
    ):
        &#34;&#34;&#34;
        Query IBM API for given ProfileCollection for personality scores.

        Allows imports and exports of results via CSV. Expected path is
        &#39;data/05&#39; + country + &#39;ibm_profiles.csv&#39;. For each profile
        in given profile collection, IBM API is called and the existing
        profile enriched by IBM personality information. Result
        is returned as enriched profile collection. If errors occur for
        a user (e.g. too few words), this user will be excluded.
        A progress bar is shown during this process. Note that IBM
        does not support all languages.

        Parameters
        ----------
        profileCol : miping.models.ProfileCollection, default=None, required
            Profile collection that should be enriched.
        country : string, default=None, required
            Country name of where the passed users are collected from
            (as specified in config)
        readFiles : boolean, default=False
            If True, CSV files will be read instead of following program
            logic.
        writeFiles : boolean, default=False
            Can only be True, if readFiles is False. If True, will export
            results to CSV files. Allows to read files in the next program
            run.

        Returns
        -------
        returnProfileCol : ProfileCollection
            New IBM enriched ProfileCollection based on the input collection.
        &#34;&#34;&#34;
        if writeFiles is True and readFiles is True:
            raise Exception(
                &#34;readFiles and writeFiles cannot be True at the same time.&#34;
            )

        returnProfileCol = ProfileCollection()

        if readFiles is True:
            print(&#34;\nReading files for get ibm profiles&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )

            # path for saved profiles
            file_directory_string = (
                &#39;data/05&#39; +
                country +
                &#39;ibm_profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.read_profile_list_file(
                full_path=file_path
            )

            print(&#34;Files successfully loaded&#34;)
        else:
            print(&#34;\nBegin getting IBM profiles&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )
            helper = Helper()

            # initialize progress bar
            numProfiles = len(profileCol.profileList)
            helper.printProgressBar(
                0,
                numProfiles,
                prefix=&#39;Progress:&#39;,
                suffix=&#39;Complete&#39;,
                length=50
            )

            # iterate over all profiles, call api
            # and enrich profiles with result
            for num, profile in enumerate(profileCol.profileList):
                profile, errorEncounterd = self.ibm.get_profile(
                    text=profile.text,
                    fillProfile=profile,
                    language=profile.language,
                )
                if errorEncounterd is False:
                    # add profile to collection
                    returnProfileCol.add_profile(profile)

                # Update Progress Bar
                helper.printProgressBar(
                    num + 1,
                    numProfiles,
                    prefix=&#39;Progress:&#39;,
                    suffix=&#39;Complete&#39;,
                    length=50
                )

            # only write file if specified
            if writeFiles is True:
                # path for saving profileCollection
                file_directory_string = (
                    &#39;data/05&#39; +
                    country +
                    &#39;ibm_profiles.csv&#39;
                )
                file_path = Path(file_directory_string)

                returnProfileCol.write_profile_list_file(
                    full_path=file_path
                )
            print(&#34;End getting IBM profiles&#34;)

        return returnProfileCol

    def do_liwc(
        self,
        profileCol,
        country,
        liwcPath,
        fileName,
        readFiles=False,
        writeFiles=False,
        skipInputWait=False
    ):
        &#34;&#34;&#34;
        Return enriched profiles with LIWC data from separate file.

        Allows imports and exports of results via CSV. Expected path is
        &#39;data/06&#39; + country + &#39;liwc_profiles.csv&#39;.
        The input profile collection contains tweet and user data and
        should be enriched with LIWC data. Since LIWC is a standalone
        program it cannot be integrated into the Python program.
        The user is asked to manually prepare the previously exported
        profile collection with LIWC and then export the LIWC results.
        These results are saved in CSV format in liwcPath and fileName.
        From there they are imported via LIWC API and enrich the existing
        profiles.

        Parameters
        ----------
        profileCol : ProfileCollection, default=None, required
            Input profile collection which should be enriched with LIWC data.
        country : string, default=None, required
            Country name of where the passed users are collected from
            (as specified in config)
        liwcPath : string, default=None, required
            Relative path of where the LIWC export is saved.
        fileName : string, default=None, required
            File name for LIWC export.
        readFiles : boolean, default=False
            If True, CSV files will be read instead of following program
            logic.
        writeFiles : boolean, default=False
            Can only be True, if readFiles is False. If True, will export
            results to CSV files. Allows to read files in the next program
            run.
        skipInputWait : boolean, default=False
            If False, the user needs to manually confirm by pressing enter
            that the LIWC export is ready for import. If True, this
            confirmation step is skipped and the program assumes the
            file already exists.

        Returns
        -------
        returnProfileCol : ProfileCollection
            New LIWC enriched ProfileCollection based on the input collection.
        &#34;&#34;&#34;

        if writeFiles is True and readFiles is True:
            raise Exception(
                &#34;readFiles and writeFiles cannot be True at the same time.&#34;
            )

        returnProfileCol = ProfileCollection()

        if readFiles is True:
            print(&#34;\nReading files for previous liwc exports&#34;)
            print(
                &#34;Loading for country: &#34; +
                country
            )

            # path for saved profiles
            file_directory_string = (
                &#39;data/06&#39; +
                country +
                &#39;liwc_profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.read_profile_list_file(
                full_path=file_path
            )

            print(&#34;Files successfully loaded&#34;)
        else:
            # path where LIWC results where saved
            file_directory_string = (
                liwcPath +
                country +
                fileName
            )
            file_path = Path(file_directory_string)

            print(
                &#34;\nBegin LIWC loading. This is a manual task. &#34; +
                &#34;Please ensure that LIWC results exist in &#34; +
                &#34;the following location: &#34; +
                str(file_directory_string)
            )
            filesReady = True
            if skipInputWait is False:
                # waiting for any user input
                # so user can create LIWC output files
                val = input(&#34;Please confirm with enter when files are ready: &#34;)
                if len(val) &gt; 0:
                    # user provided some input other than enter
                    filesReady = False
                    print(&#34;Skipping process&#34;)

            if filesReady is True:
                # import liwc results and add to profiles
                liwc = LiwcAPI()
                returnProfileCol = liwc.import_liwc_result(
                    fullPath=file_path,
                    profileCol=profileCol
                )

            # only write file if specified
            if writeFiles is True:
                # path for saving profileCollection
                file_directory_string = (
                    &#39;data/06&#39; +
                    country +
                    &#39;liwc_profiles.csv&#39;
                )
                file_path = Path(file_directory_string)

                returnProfileCol.write_profile_list_file(
                    full_path=file_path
                )
            print(&#34;End LIWC loading.&#34;)

        return returnProfileCol

    def print_statistics(
        self,
        globalProfileCollection,
    ):
        &#34;&#34;&#34;
        Print descriptive statistics for given profile collections.

        The given dictionary contains one profile collection for
        each country. Information will be printed about the number
        of users, number of words, number of tweets, and the Big
        Five personality scores.
        In the end and additional check is carried out to see if any
        user has more than 250 tweets in the collection. Since we
        selected only 250 tweets for each user, more tweets would
        indicate that we accidentally selected this user twice.

        Parameters
        ----------
        globalProfileCollection : dict, default=None, required
            Dictionary containing profile collections for each country.
            For each collection the descriptive statistics are printed.
        &#34;&#34;&#34;

        print(&#34;\nData Preparation Statistics&#34;)
        for idx, country in enumerate(globalProfileCollection):
            print(&#34;Statistics for: &#34; + str(country))

            locProfileList = globalProfileCollection[country].profileList

            # number of users
            numUsers = len(locProfileList)
            print(&#34;Number of users: &#34; + str(numUsers))

            # number of words per user
            print(&#34;Number of words&#34;)
            numWords = [
                float(getattr(profile, &#39;numberWords&#39;))
                for profile in locProfileList
            ]
            self.print_min_max_mean_std(numWords)
            # number of tweets
            print(&#34;Number of tweets&#34;)
            numTweets = [
                float(getattr(profile, &#39;numberTweets&#39;))
                for profile in locProfileList
            ]
            self.print_min_max_mean_std(numTweets)

            # ibm big 5 data
            locAttrList = [
                &#39;big5_openness&#39;,
                &#39;big5_conscientiousness&#39;,
                &#39;big5_extraversion&#39;,
                &#39;big5_agreeableness&#39;,
                &#39;big5_neuroticism&#39;
            ]

            for attr in locAttrList:
                print(attr)
                seq = []
                for profile in locProfileList:
                    value = getattr(profile, attr)
                    if value == &#39;&#39;:
                        # append nothing if no value given
                        continue
                    seq.append(float(value))
                self.print_min_max_mean_std(seq)

            # duplicate check (there should be no more than 250 tweets)
            print(&#34;User with more than 250 tweets (check for duplicates)&#34;)
            for profile in locProfileList:
                if int(getattr(profile, &#39;numberTweets&#39;)) &gt; 250:
                    print(profile)
                    print(profile.userID)
            print(
                &#34;END users with more than 250 tweets &#34; +
                &#34;(if no users shown it&#39;s fine)&#34;
            )

        print(&#34;Finished\n&#34;)
        return

    def print_min_max_mean_std(
        self,
        printList,
        percentile=False
    ):
        &#34;&#34;&#34;
        Print min, max, mean, std, and 25th and 75th percentile for values.

        Parameters
        ----------
        printList : list, default=None, required
            List containing numeric values for which descriptive statistics
            should be calculated and printed.
        percentile : boolean, default=False
            If True, additionally 25th and 75th percentile are printed.
        &#34;&#34;&#34;
        if len(printList) &gt; 0:
            print(&#34;MIN: &#34; + str(min(printList)))
            if percentile is True:
                print(&#34;25th percentile: &#34;, numpy.percentile(printList, 25))
                print(&#34;75th percentile: &#34;, numpy.percentile(printList, 75))
            print(&#34;MAX: &#34; + str(max(printList)))
            print(&#34;Mean: &#34; + str(numpy.mean(printList)))
            print(&#34;Standard Deviation: &#34; + str(numpy.std(printList)))
        else:
            print(&#39;No value in list&#39;)

        return</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="helper.preparationProcess.PreparationProcess.do_condense_tweets"><code class="name flex">
<span>def <span class="ident">do_condense_tweets</span></span>(<span>self, verifiedTweetCol, verifiedUsers, language, country, readFiles=False, writeFiles=False, hydrateUsers=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Actual data preparation for each user. Combining tweets to string.</p>
<p>This function allows to import and export results via CSV. When
importing the actual program code is skipped.
Expected CSV path: 'data/04condensed' + country + 'profiles.csv'.
It is possible to start this process with just a list of
user ids, then hydrate_users() has to be called first.
For each user in the list all tweets are combined into a single
string. Then the cleaning function is called to remove
unwanted characters. Only if the resulting string has 600 or
more space separated tokens, a Profile object is created for that
user.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>verifiedTweetCol</code></strong> :&ensp;<code>miping.models.TweetCollection</code>, default=<code>None, req</code></dt>
<dd>Previously collected tweets as tweet collection. Contains all
tweets that should be condensed and combined with the
user objects.</dd>
<dt><strong><code>verifiedUsers</code></strong> :&ensp;<code>miping.models.UserCollection</code>, default=<code>None, req</code></dt>
<dd>Previously collected users as user collection. These will be
combined with the tweet collection.</dd>
<dt><strong><code>language</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>Language (two letter ISO code) for given contry
(as specified in config).</dd>
<dt><strong><code>country</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>Country name of where the passed users are collected from
(as specified in config)</dd>
<dt><strong><code>readFiles</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>If True, CSV files will be read instead of following program
logic.</dd>
<dt><strong><code>writeFiles</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>Can only be True, if readFiles is False. If True, will export
results to CSV files. Allows to read files in the next program
run.</dd>
<dt><strong><code>hydrateUsers</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>Only True, if readFiles is False. Will call hydrate_users() and
and take its results as input for condensing. In this case
verifiedTweetCol and verifiedUsers can be passed as empty
objects.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>returnProfileCol</code></strong> :&ensp;<code>miping.models.ProfileCollection</code></dt>
<dd>Profile collection containing all profiles created.
Profiles include tweets and users.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def do_condense_tweets(
    self,
    verifiedTweetCol,
    verifiedUsers,
    language,
    country,
    readFiles=False,
    writeFiles=False,
    hydrateUsers=False,
):
    &#34;&#34;&#34;
    Actual data preparation for each user. Combining tweets to string.

    This function allows to import and export results via CSV. When
    importing the actual program code is skipped.
    Expected CSV path: &#39;data/04condensed&#39; + country + &#39;profiles.csv&#39;.
    It is possible to start this process with just a list of
    user ids, then hydrate_users() has to be called first.
    For each user in the list all tweets are combined into a single
    string. Then the cleaning function is called to remove
    unwanted characters. Only if the resulting string has 600 or
    more space separated tokens, a Profile object is created for that
    user.

    Parameters
    ----------
    verifiedTweetCol : miping.models.TweetCollection, default=None, req
        Previously collected tweets as tweet collection. Contains all
        tweets that should be condensed and combined with the
        user objects.
    verifiedUsers : miping.models.UserCollection, default=None, req
        Previously collected users as user collection. These will be
        combined with the tweet collection.
    language : string, default=None, required
        Language (two letter ISO code) for given contry
        (as specified in config).
    country : string, default=None, required
        Country name of where the passed users are collected from
        (as specified in config)
    readFiles : boolean, default=False
        If True, CSV files will be read instead of following program
        logic.
    writeFiles : boolean, default=False
        Can only be True, if readFiles is False. If True, will export
        results to CSV files. Allows to read files in the next program
        run.
    hydrateUsers : boolean, default=False
        Only True, if readFiles is False. Will call hydrate_users() and
        and take its results as input for condensing. In this case
        verifiedTweetCol and verifiedUsers can be passed as empty
        objects.

    Returns
    -------
    returnProfileCol : miping.models.ProfileCollection
        Profile collection containing all profiles created.
        Profiles include tweets and users.
    &#34;&#34;&#34;

    if writeFiles is True and readFiles is True:
        raise Exception(
            &#34;readFiles and writeFiles cannot be True at the same time.&#34;
        )

    dataPre = DataPreparation(
    )

    returnProfileCol = ProfileCollection()

    if readFiles is True:
        print(&#34;\nReading files for condense tweets&#34;)
        print(
            &#34;Loading for country: &#34; +
            country
        )

        # path for saved profiles
        file_directory_string = (
            &#39;data/04condensed&#39; +
            country +
            &#39;profiles.csv&#39;
        )
        file_path = Path(file_directory_string)

        returnProfileCol.read_profile_list_file(
            full_path=file_path
        )

        print(&#34;Files successfully loaded&#34;)
    else:
        print(&#34;\nBegin condensing tweets&#34;)
        print(&#34;Country: &#34; + str(country))

        if hydrateUsers is True:
            print(&#34;Hydrating user ids from file&#34;)
            verifiedTweetCol, verifiedUsers = self.hydrate_users(
                country=country
            )

        # iterate over all users to condense their tweets
        # and create a profile to add to profileCollection
        for user in verifiedUsers.userList:
            userID = user.id_str
            # get all saved tweets for this user
            userTweetCol = verifiedTweetCol.get_tweets_of_userid(
                userID=userID
            )
            # save number of tweets in list
            tweetCount = len(userTweetCol.tweetList)
            # combine all tweets into one string
            textString = userTweetCol.combine_tweet_text()
            # call data cleansing for combined string
            textString = dataPre.clean_text(
                textString=textString
            )

            # count words (based on any whitespace)
            wordCount = len(textString.split())

            # IBM says they need at least 600 words
            # for analysis, only then we are adding to collection
            if wordCount &gt;= 600:

                singleProfile = Profile(
                    userID=userID,
                    text=textString,
                    numberWords=wordCount,
                    numberTweets=tweetCount,
                    language=language
                )

                # add profile to collection
                returnProfileCol.add_profile(singleProfile)

        # only write file if specified
        if writeFiles is True:
            # path for saving profileCollection
            file_directory_string = (
                &#39;data/04condensed&#39; +
                country +
                &#39;profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.write_profile_list_file(
                full_path=file_path
            )
        print(&#34;End condensing tweets&#34;)

    return returnProfileCol</code></pre>
</details>
</dd>
<dt id="helper.preparationProcess.PreparationProcess.do_get_ibm_profiles"><code class="name flex">
<span>def <span class="ident">do_get_ibm_profiles</span></span>(<span>self, profileCol, country, readFiles=False, writeFiles=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Query IBM API for given ProfileCollection for personality scores.</p>
<p>Allows imports and exports of results via CSV. Expected path is
'data/05' + country + 'ibm_profiles.csv'. For each profile
in given profile collection, IBM API is called and the existing
profile enriched by IBM personality information. Result
is returned as enriched profile collection. If errors occur for
a user (e.g. too few words), this user will be excluded.
A progress bar is shown during this process. Note that IBM
does not support all languages.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>profileCol</code></strong> :&ensp;<code>miping.models.ProfileCollection</code>, default=<code>None, required</code></dt>
<dd>Profile collection that should be enriched.</dd>
<dt><strong><code>country</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>Country name of where the passed users are collected from
(as specified in config)</dd>
<dt><strong><code>readFiles</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>If True, CSV files will be read instead of following program
logic.</dd>
<dt><strong><code>writeFiles</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>Can only be True, if readFiles is False. If True, will export
results to CSV files. Allows to read files in the next program
run.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>returnProfileCol</code></strong> :&ensp;<code>ProfileCollection</code></dt>
<dd>New IBM enriched ProfileCollection based on the input collection.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def do_get_ibm_profiles(
    self,
    profileCol,
    country,
    readFiles=False,
    writeFiles=False,
):
    &#34;&#34;&#34;
    Query IBM API for given ProfileCollection for personality scores.

    Allows imports and exports of results via CSV. Expected path is
    &#39;data/05&#39; + country + &#39;ibm_profiles.csv&#39;. For each profile
    in given profile collection, IBM API is called and the existing
    profile enriched by IBM personality information. Result
    is returned as enriched profile collection. If errors occur for
    a user (e.g. too few words), this user will be excluded.
    A progress bar is shown during this process. Note that IBM
    does not support all languages.

    Parameters
    ----------
    profileCol : miping.models.ProfileCollection, default=None, required
        Profile collection that should be enriched.
    country : string, default=None, required
        Country name of where the passed users are collected from
        (as specified in config)
    readFiles : boolean, default=False
        If True, CSV files will be read instead of following program
        logic.
    writeFiles : boolean, default=False
        Can only be True, if readFiles is False. If True, will export
        results to CSV files. Allows to read files in the next program
        run.

    Returns
    -------
    returnProfileCol : ProfileCollection
        New IBM enriched ProfileCollection based on the input collection.
    &#34;&#34;&#34;
    if writeFiles is True and readFiles is True:
        raise Exception(
            &#34;readFiles and writeFiles cannot be True at the same time.&#34;
        )

    returnProfileCol = ProfileCollection()

    if readFiles is True:
        print(&#34;\nReading files for get ibm profiles&#34;)
        print(
            &#34;Loading for country: &#34; +
            country
        )

        # path for saved profiles
        file_directory_string = (
            &#39;data/05&#39; +
            country +
            &#39;ibm_profiles.csv&#39;
        )
        file_path = Path(file_directory_string)

        returnProfileCol.read_profile_list_file(
            full_path=file_path
        )

        print(&#34;Files successfully loaded&#34;)
    else:
        print(&#34;\nBegin getting IBM profiles&#34;)
        print(
            &#34;Loading for country: &#34; +
            country
        )
        helper = Helper()

        # initialize progress bar
        numProfiles = len(profileCol.profileList)
        helper.printProgressBar(
            0,
            numProfiles,
            prefix=&#39;Progress:&#39;,
            suffix=&#39;Complete&#39;,
            length=50
        )

        # iterate over all profiles, call api
        # and enrich profiles with result
        for num, profile in enumerate(profileCol.profileList):
            profile, errorEncounterd = self.ibm.get_profile(
                text=profile.text,
                fillProfile=profile,
                language=profile.language,
            )
            if errorEncounterd is False:
                # add profile to collection
                returnProfileCol.add_profile(profile)

            # Update Progress Bar
            helper.printProgressBar(
                num + 1,
                numProfiles,
                prefix=&#39;Progress:&#39;,
                suffix=&#39;Complete&#39;,
                length=50
            )

        # only write file if specified
        if writeFiles is True:
            # path for saving profileCollection
            file_directory_string = (
                &#39;data/05&#39; +
                country +
                &#39;ibm_profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.write_profile_list_file(
                full_path=file_path
            )
        print(&#34;End getting IBM profiles&#34;)

    return returnProfileCol</code></pre>
</details>
</dd>
<dt id="helper.preparationProcess.PreparationProcess.do_liwc"><code class="name flex">
<span>def <span class="ident">do_liwc</span></span>(<span>self, profileCol, country, liwcPath, fileName, readFiles=False, writeFiles=False, skipInputWait=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return enriched profiles with LIWC data from separate file.</p>
<p>Allows imports and exports of results via CSV. Expected path is
'data/06' + country + 'liwc_profiles.csv'.
The input profile collection contains tweet and user data and
should be enriched with LIWC data. Since LIWC is a standalone
program it cannot be integrated into the Python program.
The user is asked to manually prepare the previously exported
profile collection with LIWC and then export the LIWC results.
These results are saved in CSV format in liwcPath and fileName.
From there they are imported via LIWC API and enrich the existing
profiles.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>profileCol</code></strong> :&ensp;<code>ProfileCollection</code>, default=<code>None, required</code></dt>
<dd>Input profile collection which should be enriched with LIWC data.</dd>
<dt><strong><code>country</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>Country name of where the passed users are collected from
(as specified in config)</dd>
<dt><strong><code>liwcPath</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>Relative path of where the LIWC export is saved.</dd>
<dt><strong><code>fileName</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>File name for LIWC export.</dd>
<dt><strong><code>readFiles</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>If True, CSV files will be read instead of following program
logic.</dd>
<dt><strong><code>writeFiles</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>Can only be True, if readFiles is False. If True, will export
results to CSV files. Allows to read files in the next program
run.</dd>
<dt><strong><code>skipInputWait</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>If False, the user needs to manually confirm by pressing enter
that the LIWC export is ready for import. If True, this
confirmation step is skipped and the program assumes the
file already exists.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>returnProfileCol</code></strong> :&ensp;<code>ProfileCollection</code></dt>
<dd>New LIWC enriched ProfileCollection based on the input collection.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def do_liwc(
    self,
    profileCol,
    country,
    liwcPath,
    fileName,
    readFiles=False,
    writeFiles=False,
    skipInputWait=False
):
    &#34;&#34;&#34;
    Return enriched profiles with LIWC data from separate file.

    Allows imports and exports of results via CSV. Expected path is
    &#39;data/06&#39; + country + &#39;liwc_profiles.csv&#39;.
    The input profile collection contains tweet and user data and
    should be enriched with LIWC data. Since LIWC is a standalone
    program it cannot be integrated into the Python program.
    The user is asked to manually prepare the previously exported
    profile collection with LIWC and then export the LIWC results.
    These results are saved in CSV format in liwcPath and fileName.
    From there they are imported via LIWC API and enrich the existing
    profiles.

    Parameters
    ----------
    profileCol : ProfileCollection, default=None, required
        Input profile collection which should be enriched with LIWC data.
    country : string, default=None, required
        Country name of where the passed users are collected from
        (as specified in config)
    liwcPath : string, default=None, required
        Relative path of where the LIWC export is saved.
    fileName : string, default=None, required
        File name for LIWC export.
    readFiles : boolean, default=False
        If True, CSV files will be read instead of following program
        logic.
    writeFiles : boolean, default=False
        Can only be True, if readFiles is False. If True, will export
        results to CSV files. Allows to read files in the next program
        run.
    skipInputWait : boolean, default=False
        If False, the user needs to manually confirm by pressing enter
        that the LIWC export is ready for import. If True, this
        confirmation step is skipped and the program assumes the
        file already exists.

    Returns
    -------
    returnProfileCol : ProfileCollection
        New LIWC enriched ProfileCollection based on the input collection.
    &#34;&#34;&#34;

    if writeFiles is True and readFiles is True:
        raise Exception(
            &#34;readFiles and writeFiles cannot be True at the same time.&#34;
        )

    returnProfileCol = ProfileCollection()

    if readFiles is True:
        print(&#34;\nReading files for previous liwc exports&#34;)
        print(
            &#34;Loading for country: &#34; +
            country
        )

        # path for saved profiles
        file_directory_string = (
            &#39;data/06&#39; +
            country +
            &#39;liwc_profiles.csv&#39;
        )
        file_path = Path(file_directory_string)

        returnProfileCol.read_profile_list_file(
            full_path=file_path
        )

        print(&#34;Files successfully loaded&#34;)
    else:
        # path where LIWC results where saved
        file_directory_string = (
            liwcPath +
            country +
            fileName
        )
        file_path = Path(file_directory_string)

        print(
            &#34;\nBegin LIWC loading. This is a manual task. &#34; +
            &#34;Please ensure that LIWC results exist in &#34; +
            &#34;the following location: &#34; +
            str(file_directory_string)
        )
        filesReady = True
        if skipInputWait is False:
            # waiting for any user input
            # so user can create LIWC output files
            val = input(&#34;Please confirm with enter when files are ready: &#34;)
            if len(val) &gt; 0:
                # user provided some input other than enter
                filesReady = False
                print(&#34;Skipping process&#34;)

        if filesReady is True:
            # import liwc results and add to profiles
            liwc = LiwcAPI()
            returnProfileCol = liwc.import_liwc_result(
                fullPath=file_path,
                profileCol=profileCol
            )

        # only write file if specified
        if writeFiles is True:
            # path for saving profileCollection
            file_directory_string = (
                &#39;data/06&#39; +
                country +
                &#39;liwc_profiles.csv&#39;
            )
            file_path = Path(file_directory_string)

            returnProfileCol.write_profile_list_file(
                full_path=file_path
            )
        print(&#34;End LIWC loading.&#34;)

    return returnProfileCol</code></pre>
</details>
</dd>
<dt id="helper.preparationProcess.PreparationProcess.hydrate_users"><code class="name flex">
<span>def <span class="ident">hydrate_users</span></span>(<span>self, country)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetch and return user profiles and tweet list for user ID list
in CSV file.</p>
<p>Twitter allows only to pass tweet and user IDs, when publicly sharing
scraped data. This function allows to import a CSV file containing
only user IDs and automatically fetching related user objects and
for each user up to the latest 250 tweets. Private users are skipped.
The CSV file path is hard coded.
Expected CSV path: 'data/04' + country + 'UserIDs.csv'</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>country</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>The country parameter is used to differentiate between CSV files.
The country name is automatically added to the file name. It
should match a country given in the global config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tweetCol</code></strong> :&ensp;<code>miping.models.TweetCollection</code></dt>
<dd>Data model type in miping module. Contains the retrieved tweets
for all given users.</dd>
<dt><strong><code>usersCol</code></strong> :&ensp;<code>miping.models.UserCollection</code></dt>
<dd>Data model type in miping module. Contains user objects as
returned by Twitter API.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hydrate_users(
    self,
    country,
):
    &#34;&#34;&#34;
    Fetch and return user profiles and tweet list for user ID list
    in CSV file.

    Twitter allows only to pass tweet and user IDs, when publicly sharing
    scraped data. This function allows to import a CSV file containing
    only user IDs and automatically fetching related user objects and
    for each user up to the latest 250 tweets. Private users are skipped.
    The CSV file path is hard coded.
    Expected CSV path: &#39;data/04&#39; + country + &#39;UserIDs.csv&#39;

    Parameters
    ----------
    country : string, default=None, required
        The country parameter is used to differentiate between CSV files.
        The country name is automatically added to the file name. It
        should match a country given in the global config.

    Returns
    -------
    tweetCol : miping.models.TweetCollection
        Data model type in miping module. Contains the retrieved tweets
        for all given users.
    usersCol : miping.models.UserCollection
        Data model type in miping module. Contains user objects as
        returned by Twitter API.
    &#34;&#34;&#34;
    # load user ids from file
    # path for saved profiles
    file_directory_string = (
        &#39;data/04&#39; +
        country +
        &#39;UserIDs.csv&#39;
    )
    file_path = Path(file_directory_string)

    # read in user ids
    profileCol = ProfileCollection()
    profileCol.read_profile_list_file(
        full_path=file_path,
        idsonly=True
    )

    # extract userIDs as list
    idList = [obj.userID for obj in profileCol.profileList]
    # create user object
    usersCol = self.twitter.getUsersByList(idList)

    # create tweet collection for return
    tweetCol = TweetCollection(
        additionalAttributes=self.config[&#34;twitter&#34;][&#34;add_attributes&#34;]
    )
    for user in usersCol.userList:
        # get tweets for given user
        singleTweetCol = self.twitter.funcGetTweetListByUser(
                userID=user.id_str,
                limit=250,
        )
        # add to total collection
        tweetCol.add_tweet_collection(singleTweetCol)

    print(&#34;Hydration finished&#34;)

    return tweetCol, usersCol</code></pre>
</details>
</dd>
<dt id="helper.preparationProcess.PreparationProcess.print_min_max_mean_std"><code class="name flex">
<span>def <span class="ident">print_min_max_mean_std</span></span>(<span>self, printList, percentile=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Print min, max, mean, std, and 25th and 75th percentile for values.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>printList</code></strong> :&ensp;<code>list</code>, default=<code>None, required</code></dt>
<dd>List containing numeric values for which descriptive statistics
should be calculated and printed.</dd>
<dt><strong><code>percentile</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>If True, additionally 25th and 75th percentile are printed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_min_max_mean_std(
    self,
    printList,
    percentile=False
):
    &#34;&#34;&#34;
    Print min, max, mean, std, and 25th and 75th percentile for values.

    Parameters
    ----------
    printList : list, default=None, required
        List containing numeric values for which descriptive statistics
        should be calculated and printed.
    percentile : boolean, default=False
        If True, additionally 25th and 75th percentile are printed.
    &#34;&#34;&#34;
    if len(printList) &gt; 0:
        print(&#34;MIN: &#34; + str(min(printList)))
        if percentile is True:
            print(&#34;25th percentile: &#34;, numpy.percentile(printList, 25))
            print(&#34;75th percentile: &#34;, numpy.percentile(printList, 75))
        print(&#34;MAX: &#34; + str(max(printList)))
        print(&#34;Mean: &#34; + str(numpy.mean(printList)))
        print(&#34;Standard Deviation: &#34; + str(numpy.std(printList)))
    else:
        print(&#39;No value in list&#39;)

    return</code></pre>
</details>
</dd>
<dt id="helper.preparationProcess.PreparationProcess.print_statistics"><code class="name flex">
<span>def <span class="ident">print_statistics</span></span>(<span>self, globalProfileCollection)</span>
</code></dt>
<dd>
<div class="desc"><p>Print descriptive statistics for given profile collections.</p>
<p>The given dictionary contains one profile collection for
each country. Information will be printed about the number
of users, number of words, number of tweets, and the Big
Five personality scores.
In the end and additional check is carried out to see if any
user has more than 250 tweets in the collection. Since we
selected only 250 tweets for each user, more tweets would
indicate that we accidentally selected this user twice.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>globalProfileCollection</code></strong> :&ensp;<code>dict</code>, default=<code>None, required</code></dt>
<dd>Dictionary containing profile collections for each country.
For each collection the descriptive statistics are printed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_statistics(
    self,
    globalProfileCollection,
):
    &#34;&#34;&#34;
    Print descriptive statistics for given profile collections.

    The given dictionary contains one profile collection for
    each country. Information will be printed about the number
    of users, number of words, number of tweets, and the Big
    Five personality scores.
    In the end and additional check is carried out to see if any
    user has more than 250 tweets in the collection. Since we
    selected only 250 tweets for each user, more tweets would
    indicate that we accidentally selected this user twice.

    Parameters
    ----------
    globalProfileCollection : dict, default=None, required
        Dictionary containing profile collections for each country.
        For each collection the descriptive statistics are printed.
    &#34;&#34;&#34;

    print(&#34;\nData Preparation Statistics&#34;)
    for idx, country in enumerate(globalProfileCollection):
        print(&#34;Statistics for: &#34; + str(country))

        locProfileList = globalProfileCollection[country].profileList

        # number of users
        numUsers = len(locProfileList)
        print(&#34;Number of users: &#34; + str(numUsers))

        # number of words per user
        print(&#34;Number of words&#34;)
        numWords = [
            float(getattr(profile, &#39;numberWords&#39;))
            for profile in locProfileList
        ]
        self.print_min_max_mean_std(numWords)
        # number of tweets
        print(&#34;Number of tweets&#34;)
        numTweets = [
            float(getattr(profile, &#39;numberTweets&#39;))
            for profile in locProfileList
        ]
        self.print_min_max_mean_std(numTweets)

        # ibm big 5 data
        locAttrList = [
            &#39;big5_openness&#39;,
            &#39;big5_conscientiousness&#39;,
            &#39;big5_extraversion&#39;,
            &#39;big5_agreeableness&#39;,
            &#39;big5_neuroticism&#39;
        ]

        for attr in locAttrList:
            print(attr)
            seq = []
            for profile in locProfileList:
                value = getattr(profile, attr)
                if value == &#39;&#39;:
                    # append nothing if no value given
                    continue
                seq.append(float(value))
            self.print_min_max_mean_std(seq)

        # duplicate check (there should be no more than 250 tweets)
        print(&#34;User with more than 250 tweets (check for duplicates)&#34;)
        for profile in locProfileList:
            if int(getattr(profile, &#39;numberTweets&#39;)) &gt; 250:
                print(profile)
                print(profile.userID)
        print(
            &#34;END users with more than 250 tweets &#34; +
            &#34;(if no users shown it&#39;s fine)&#34;
        )

    print(&#34;Finished\n&#34;)
    return</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="helper" href="index.html">helper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="helper.preparationProcess.PreparationProcess" href="#helper.preparationProcess.PreparationProcess">PreparationProcess</a></code></h4>
<ul class="">
<li><code><a title="helper.preparationProcess.PreparationProcess.do_condense_tweets" href="#helper.preparationProcess.PreparationProcess.do_condense_tweets">do_condense_tweets</a></code></li>
<li><code><a title="helper.preparationProcess.PreparationProcess.do_get_ibm_profiles" href="#helper.preparationProcess.PreparationProcess.do_get_ibm_profiles">do_get_ibm_profiles</a></code></li>
<li><code><a title="helper.preparationProcess.PreparationProcess.do_liwc" href="#helper.preparationProcess.PreparationProcess.do_liwc">do_liwc</a></code></li>
<li><code><a title="helper.preparationProcess.PreparationProcess.hydrate_users" href="#helper.preparationProcess.PreparationProcess.hydrate_users">hydrate_users</a></code></li>
<li><code><a title="helper.preparationProcess.PreparationProcess.print_min_max_mean_std" href="#helper.preparationProcess.PreparationProcess.print_min_max_mean_std">print_min_max_mean_std</a></code></li>
<li><code><a title="helper.preparationProcess.PreparationProcess.print_statistics" href="#helper.preparationProcess.PreparationProcess.print_statistics">print_statistics</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>