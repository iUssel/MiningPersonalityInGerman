<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>miping.training.modelTraining API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>miping.training.modelTraining</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import numpy as np

from ..models.modelBase import ModelBase

from warnings import simplefilter

from sklearn.exceptions import ConvergenceWarning
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate


class ModelTraining:
    &#34;&#34;&#34;
    Wrapper class for all model training functions.
    &#34;&#34;&#34;

    def __init__(
        self,
        labelsGlobalList,
        crossValidationIterations=10,
        n_jobs=1,
        # e.g. neg_mean_absolute_error MAE or
        # neg_root_mean_squared_error RMSE
        scoringFunc=&#39;neg_root_mean_squared_error&#39;,
        printIntermediateResults=True,
        printCoefficients=False,
    ):
        &#34;&#34;&#34;
        Initialize values for model training.

        Parameters
        ----------
        labelsGlobalList : list, default=None, required
            List of dimensions to predict (e.g. Big Five).
        crossValidationIterations : integer, default=10
            Number of cross validation iterations.
        n_jobs : integer, default=1
            Number of prallel jobs in sklearn.
            n_jobs=-1 -&gt; parallelization. Does not always work.
            E.g. SVM throws warnings.
        scoringFunc : string, default=&#39;neg_root_mean_squared_error&#39;
            Which scoring function should be chosen for selecting
            the best model. Should be sklearn function. Since sklearn
            follows &#34;higher = better&#34;. Error scores are negative.
        printIntermediateResults : boolean, default=True
            Print intermediate results, such as scores.
        printCoefficients : boolean, default=False
            Print model coefficients and kernel parameters of trained models.
        &#34;&#34;&#34;
        # save list of labels to predict
        # one model for each label will be calculated
        self.labelsGlobalList = labelsGlobalList

        # number of iterations during cross validations
        self.crossValidationIterations = crossValidationIterations

        # 1 if no parallelization, -1 all CPU cores
        self.n_jobs = n_jobs

        # scoring function used during cross validation
        self.scoringFunc = scoringFunc

        # to keep the terminal output clean, prints can
        # be minimized, showing only most relevant prints
        self.printIntermediateResults = printIntermediateResults

        # some model&#39;s have coefficients that are
        # set during training and might be of interest
        self.printCoefficients = printCoefficients

        return

    def extractLabels(
        self,
        profileList,
        labelName,
    ):
        &#34;&#34;&#34;
        Extract values for one specific labelName
        labels are the percentages value to predict
        e.g. for Extraversion value.

        Parameters
        ----------
        profileList : list, default=None, required
            List of profiles to extract labels from.
        labelName : string, default=None, required
            Label to extract from profiles.

        Returns
        -------
        labels : list
            Numeric values used for training and prediction.
        &#34;&#34;&#34;
        # initialize return variable
        labels = []

        # loop over profile collection
        for profile in profileList:
            value = getattr(profile, labelName)
            labels.append(np.float(value))

        return labels

    def crossvalidateModel(
        self,
        model,
        gridSearchParams,
        labels,
        features
    ):
        &#34;&#34;&#34;
        Return best model from grid search and do cross validation.

        A grid search is performed with the given parameters.
        The best model selected and a cross validation with
        this model performed. Scores will be printed to console.
        Duration of process is printed.

        Parameters
        ----------
        model : modelBase.model, default=None, required
            Model from sklearn to train.
        gridSearchParams : dict, default=None, required
            Dictionary with parameters for grid search.
        labels : list, default=None, required
            Numeric values as prediction target.
        features : numpy.array, default=None, required
            Numeric features to predict from.

        Returns
        -------grid_model.best_score_, bestModel, bestParams
        grid_model.best_score_ : dict
            Dictionary with scores of best model.
        bestModel : modelBase.model
            Best performing model during gridsearch.
        bestParams : dict
            Parameters of best performing model.
        &#34;&#34;&#34;

        # ignore terminated early warning
        # for SVMs we set a hard limit for max iterations
        # and it would print this warning once the iterations
        # are reached.
        # somehow a bug in scikit prevents ignoring
        # when running job in parallel
        simplefilter(&#34;ignore&#34;, category=ConvergenceWarning)

        # save start time, for runtime calculation
        startTime = datetime.datetime.now()

        # set up cross validation with params
        grid_model = GridSearchCV(
            model,
            gridSearchParams,
            cv=self.crossValidationIterations,
            n_jobs=self.n_jobs,
            scoring=self.scoringFunc  # we usually use RMSE
        )

        # number of datasets available
        n = len(labels)

        # actually do grid search
        grid_model = grid_model.fit(features[:n], labels[:n])

        if self.printIntermediateResults is True:
            print(&#34;best score: &#34; + str(grid_model.best_score_))
        bestParams = {}
        for param_name in sorted(gridSearchParams.keys()):
            bestParams[param_name] = grid_model.best_params_[param_name]
            if self.printIntermediateResults is True:
                print(
                    &#34;%s: %r&#34; %
                    (param_name, grid_model.best_params_[param_name])
                )

        # set classifier to best classifier found by GridSearchCV
        bestModel = grid_model.best_estimator_

        # cross validation
        scores = cross_val_score(
            bestModel,
            features,
            labels,
            cv=self.crossValidationIterations,
            n_jobs=self.n_jobs,
            scoring=self.scoringFunc
        )
        if self.printIntermediateResults is True:
            print(
                self.scoringFunc +
                &#34; Score: %0.4f (+/- %0.4f)&#34; %
                (scores.mean(), scores.std() * 2)
            )
        if self.printCoefficients is True:
            if hasattr(bestModel, &#39;coef_&#39;):
                # some models provide coefficients
                # that are set during training
                print(
                    &#34;Model&#39;s coefficients: &#34; +
                    str(bestModel.coef_)
                )
            if hasattr(bestModel, &#39;kernel_&#39;):
                # gaussian proccesses have kernel parameters
                print(
                    &#34;Kernel parameters: &#34; +
                    str(bestModel.kernel_)
                )

        endTime = datetime.datetime.now()
        runTime = endTime - startTime
        if self.printIntermediateResults is True:
            print(&#34;Duration: &#34; + str(runTime))

        return grid_model.best_score_, bestModel, bestParams

    def provideScores(
        self,
        modelBase,
        labels,
        features
    ):
        &#34;&#34;&#34;
        Does cross validation for given model in modelBase.
        Calculates MAE, MSE, RMSE, correlations and R2.
        Prints results and saves in modelBase.

        Parameters
        ----------
        modelBase : modelBase, default=None, required
            ModelBase class containing the model.
        labels : list, default=None, required
            Numeric values as prediction target.
        features : numpy.array, default=None, required
            Numeric features to predict from.
        &#34;&#34;&#34;
        # get actual estimator from object
        model = modelBase.model
        # summarize all metrics
        scoring = {
            &#39;negMAE&#39;: &#39;neg_mean_absolute_error&#39;,
            &#39;negMSE&#39;: &#39;neg_mean_squared_error&#39;,
            &#39;negRMSE&#39;: &#39;neg_root_mean_squared_error&#39;,
            &#39;R2&#39;: &#39;r2&#39;,
        }

        # cross validation to estimate scores again
        cv_results = cross_validate(
            model,
            features,
            labels,
            cv=self.crossValidationIterations,
            n_jobs=self.n_jobs,
            scoring=scoring
        )

        scores_to_save = {}

        # Printing scores for each model
        for key in scoring.keys():
            # get results, these are prefixed with test_
            key_value = (&#39;test_&#39; + str(key))
            scores = cv_results[key_value]
            values = &#34;%0.4f (+/- %0.4f)&#34; % (scores.mean(), scores.std() * 2)
            if self.printIntermediateResults is True:
                print(
                    str(key) + &#34;: &#34; +
                    str(values)
                )
            # save scores to save it later in model object
            scores_to_save[key] = values

        # save scores in model object
        modelBase.scores = scores_to_save

        return

    def startModelSelection(
        self,
        modelObjList,
        featurePipeline,
        profileColTraining,
        saveFeatures=False,
        precalculatedFeatures=None,
    ):
        &#34;&#34;&#34;
        Identify best model for each dimension.

        First, features are calculated. The model selection is done
        for each dimension (label). Labels are extracted and for each
        model in modelObjList grid search is performed.
        Then best model is identified for this dimension and added to
        globalBestModels.
        Additional scores are provided via cross validation.

        Parameters
        ----------
        modelObjList : list, default=None, required
            List of models to select from.
        featurePipeline : Pipeline, default=None, required
            Created feature pipeline to use.
        profileColTraining : ProfileCollection, default=None, required
            ProfileCollection to generate features and extract labels from.
        saveFeatures : boolean, default=False
            To save time feature calculation step can be saved and
            exported in caller function.
        precalculatedFeatures : numpy array, default=None
            If passed, those features are used and no feature calculation
            takes place.

        Returns
        -------
        globalBestModels : dict
            Dictionary with best model for each label to predict.
        &#34;&#34;&#34;
        profileList = profileColTraining.profileList

        if precalculatedFeatures is None:
            # calculate features once
            print(&#34;Calculating features&#34;)
            features = featurePipeline.fit_transform(profileList)
        else:
            print(&#34;Features from passed variable&#34;)
            features = precalculatedFeatures

        if saveFeatures is True:
            self.features = features

        print(&#34;Feature shape: &#34; + str(features.shape))
        self.features = features

        globalBestModels = {}
        scoreStatistics = {}

        # do this whole process for each label
        # (e.g. big dimensions or big 5 facets)
        for labelName in self.labelsGlobalList:
            print(
                &#34;\nModel Selection currently for label: &#34; +
                str(labelName)
            )
            # extract labels for prediction (e.g. values for Extraversion)
            labels = self.extractLabels(
                profileList=profileList,
                labelName=labelName
            )

            labelBestModels = {}

            # for each model in model list, we will perform grid search
            for modelObj in modelObjList:
                # get model
                currentModel = modelObj.getModel()
                currentGridParams = modelObj.gridSearchParams
                print(&#39;\nCurrently at model: &#39; + modelObj.modelName)
                # do crossvalidation for this model

                best_score, bestModel, bestParams = self.crossvalidateModel(
                    model=currentModel,
                    gridSearchParams=currentGridParams,
                    labels=labels,
                    features=features,
                )
                # save best model as object in dict, to select from
                # after all have been evaluated
                localModel = ModelBase(
                    labelName=labelName,
                    modelName=modelObj.modelName,
                    model=bestModel,
                    params=bestParams,
                    scores=best_score,
                    gridSearchParams=None,
                )
                labelBestModels[modelObj.modelName] = localModel
                # save score in dict, to evalute each model type
                # at the end of the function
                self._nested_set(
                    scoreStatistics,
                    [modelObj.modelName, labelName],
                    best_score
                )

            # select best model and its params by taking entry
            # with highest score
            bestModelName = max(
                labelBestModels.keys(),
                key=(lambda k: labelBestModels[k].scores)
            )
            # save it for this label in dict
            globalBestModels[labelName] = labelBestModels[bestModelName]

            # print model name
            print(
                &#34;\nBest model for &#34; +
                labelName +
                &#34; is &#34; +
                globalBestModels[labelName].modelName
            )

            # provide MAE, MSE, RMSE, correlations and R2
            # for comparison reasons
            self.provideScores(
                globalBestModels[labelName],
                labels,
                features
            )

        # for comparison identify model, that on average performs
        # best over all labels
        # meaning good scores in all dimensions for this model type
        # helps with model selection and avoid overfitting
        if self.printIntermediateResults is True:
            print(
                &#34;\nOverall average performance &#34; +
                &#34;(might help with model selection):&#34;
            )
            meanScores = {}
            for modelType in scoreStatistics:
                scores = 0
                counter = 0
                for score in scoreStatistics[modelType].values():
                    scores = scores + float(score)
                    counter = counter + 1
                mean = scores / counter
                meanScores[modelType] = mean
                print(
                    &#34;Mean score over all dimensions for model type &#34; +
                    str(modelType) +
                    &#34; is :&#34; +
                    str(mean)
                )
            # best mean score
            meanBest = max(
                meanScores.keys(),
                key=(lambda k: meanScores[k])
            )
            print(&#34;Mean best is &#34; + str(meanBest))

        return globalBestModels

    def completeModelTraining(
        self,
        modelCollection,
        featurePipeline,
        profileColTraining,
        saveFeatures=False,
        precalculatedFeatures=None,
    ):
        &#34;&#34;&#34;
        Fully train models in modelCollection.

        Features are calculated, then for each dimension model is
        fully trained and saved back in collection.

        Parameters
        ----------
        modelCollection : dict, default=None, required
            Dictionary of best models to fully train.
        featurePipeline : Pipeline, default=None, required
            Created feature pipeline to use.
        profileColTraining : ProfileCollection, default=None, required
            ProfileCollection to generate features and extract labels from.
        saveFeatures : boolean, default=False
            To save time feature calculation step can be saved and
            exported in caller function.
        precalculatedFeatures : numpy array, default=None
            If passed, those features are used and no feature calculation
            takes place.

        Returns
        -------
        modelCollection : dict
            Collection contains now fully trained models.
        &#34;&#34;&#34;
        profileList = profileColTraining.profileList

        if precalculatedFeatures is None:
            # calculate features once
            print(&#34;Calculating features for complete training&#34;)
            features = featurePipeline.fit_transform(profileList)
        else:
            print(&#34;Features from passed variable&#34;)
            features = precalculatedFeatures

        if saveFeatures is True:
            self.features = features

        for labelName in self.labelsGlobalList:
            print(
                &#34;Full Model Training currently for label: &#34; +
                str(labelName)
            )
            # extract labels for prediction (e.g. values for Extraversion)
            labels = self.extractLabels(
                profileList=profileList,
                labelName=labelName
            )

            # select the model we previously evaluated for this trait
            baseModel = modelCollection[labelName]
            model = baseModel.model

            # for each model, we will fully train the model
            model.fit(features, labels)

            # save model back in object
            baseModel.model = model
            # back into collection
            modelCollection[labelName] = baseModel

        return modelCollection

    def _nested_set(
        self,
        dic,
        keys,
        value
    ):
        &#34;&#34;&#34;
        Helps when saving scores to intialize
        dictionary.
        nested arrays
        https://stackoverflow.com/questions/13687924/
        setting-a-value-in-a-nested-python-dictionary-
        given-a-list-of-indices-and-value
        &#34;&#34;&#34;
        for key in keys[:-1]:
            dic = dic.setdefault(key, {})
        dic[keys[-1]] = value</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="miping.training.modelTraining.ModelTraining"><code class="flex name class">
<span>class <span class="ident">ModelTraining</span></span>
<span>(</span><span>labelsGlobalList, crossValidationIterations=10, n_jobs=1, scoringFunc='neg_root_mean_squared_error', printIntermediateResults=True, printCoefficients=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrapper class for all model training functions.</p>
<p>Initialize values for model training.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>labelsGlobalList</code></strong> :&ensp;<code>list</code>, default=<code>None, required</code></dt>
<dd>List of dimensions to predict (e.g. Big Five).</dd>
<dt><strong><code>crossValidationIterations</code></strong> :&ensp;<code>integer</code>, default=<code>10</code></dt>
<dd>Number of cross validation iterations.</dd>
<dt><strong><code>n_jobs</code></strong> :&ensp;<code>integer</code>, default=<code>1</code></dt>
<dd>Number of prallel jobs in sklearn.
n_jobs=-1 -&gt; parallelization. Does not always work.
E.g. SVM throws warnings.</dd>
<dt><strong><code>scoringFunc</code></strong> :&ensp;<code>string</code>, default=<code>'neg_root_mean_squared_error'</code></dt>
<dd>Which scoring function should be chosen for selecting
the best model. Should be sklearn function. Since sklearn
follows "higher = better". Error scores are negative.</dd>
<dt><strong><code>printIntermediateResults</code></strong> :&ensp;<code>boolean</code>, default=<code>True</code></dt>
<dd>Print intermediate results, such as scores.</dd>
<dt><strong><code>printCoefficients</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>Print model coefficients and kernel parameters of trained models.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelTraining:
    &#34;&#34;&#34;
    Wrapper class for all model training functions.
    &#34;&#34;&#34;

    def __init__(
        self,
        labelsGlobalList,
        crossValidationIterations=10,
        n_jobs=1,
        # e.g. neg_mean_absolute_error MAE or
        # neg_root_mean_squared_error RMSE
        scoringFunc=&#39;neg_root_mean_squared_error&#39;,
        printIntermediateResults=True,
        printCoefficients=False,
    ):
        &#34;&#34;&#34;
        Initialize values for model training.

        Parameters
        ----------
        labelsGlobalList : list, default=None, required
            List of dimensions to predict (e.g. Big Five).
        crossValidationIterations : integer, default=10
            Number of cross validation iterations.
        n_jobs : integer, default=1
            Number of prallel jobs in sklearn.
            n_jobs=-1 -&gt; parallelization. Does not always work.
            E.g. SVM throws warnings.
        scoringFunc : string, default=&#39;neg_root_mean_squared_error&#39;
            Which scoring function should be chosen for selecting
            the best model. Should be sklearn function. Since sklearn
            follows &#34;higher = better&#34;. Error scores are negative.
        printIntermediateResults : boolean, default=True
            Print intermediate results, such as scores.
        printCoefficients : boolean, default=False
            Print model coefficients and kernel parameters of trained models.
        &#34;&#34;&#34;
        # save list of labels to predict
        # one model for each label will be calculated
        self.labelsGlobalList = labelsGlobalList

        # number of iterations during cross validations
        self.crossValidationIterations = crossValidationIterations

        # 1 if no parallelization, -1 all CPU cores
        self.n_jobs = n_jobs

        # scoring function used during cross validation
        self.scoringFunc = scoringFunc

        # to keep the terminal output clean, prints can
        # be minimized, showing only most relevant prints
        self.printIntermediateResults = printIntermediateResults

        # some model&#39;s have coefficients that are
        # set during training and might be of interest
        self.printCoefficients = printCoefficients

        return

    def extractLabels(
        self,
        profileList,
        labelName,
    ):
        &#34;&#34;&#34;
        Extract values for one specific labelName
        labels are the percentages value to predict
        e.g. for Extraversion value.

        Parameters
        ----------
        profileList : list, default=None, required
            List of profiles to extract labels from.
        labelName : string, default=None, required
            Label to extract from profiles.

        Returns
        -------
        labels : list
            Numeric values used for training and prediction.
        &#34;&#34;&#34;
        # initialize return variable
        labels = []

        # loop over profile collection
        for profile in profileList:
            value = getattr(profile, labelName)
            labels.append(np.float(value))

        return labels

    def crossvalidateModel(
        self,
        model,
        gridSearchParams,
        labels,
        features
    ):
        &#34;&#34;&#34;
        Return best model from grid search and do cross validation.

        A grid search is performed with the given parameters.
        The best model selected and a cross validation with
        this model performed. Scores will be printed to console.
        Duration of process is printed.

        Parameters
        ----------
        model : modelBase.model, default=None, required
            Model from sklearn to train.
        gridSearchParams : dict, default=None, required
            Dictionary with parameters for grid search.
        labels : list, default=None, required
            Numeric values as prediction target.
        features : numpy.array, default=None, required
            Numeric features to predict from.

        Returns
        -------grid_model.best_score_, bestModel, bestParams
        grid_model.best_score_ : dict
            Dictionary with scores of best model.
        bestModel : modelBase.model
            Best performing model during gridsearch.
        bestParams : dict
            Parameters of best performing model.
        &#34;&#34;&#34;

        # ignore terminated early warning
        # for SVMs we set a hard limit for max iterations
        # and it would print this warning once the iterations
        # are reached.
        # somehow a bug in scikit prevents ignoring
        # when running job in parallel
        simplefilter(&#34;ignore&#34;, category=ConvergenceWarning)

        # save start time, for runtime calculation
        startTime = datetime.datetime.now()

        # set up cross validation with params
        grid_model = GridSearchCV(
            model,
            gridSearchParams,
            cv=self.crossValidationIterations,
            n_jobs=self.n_jobs,
            scoring=self.scoringFunc  # we usually use RMSE
        )

        # number of datasets available
        n = len(labels)

        # actually do grid search
        grid_model = grid_model.fit(features[:n], labels[:n])

        if self.printIntermediateResults is True:
            print(&#34;best score: &#34; + str(grid_model.best_score_))
        bestParams = {}
        for param_name in sorted(gridSearchParams.keys()):
            bestParams[param_name] = grid_model.best_params_[param_name]
            if self.printIntermediateResults is True:
                print(
                    &#34;%s: %r&#34; %
                    (param_name, grid_model.best_params_[param_name])
                )

        # set classifier to best classifier found by GridSearchCV
        bestModel = grid_model.best_estimator_

        # cross validation
        scores = cross_val_score(
            bestModel,
            features,
            labels,
            cv=self.crossValidationIterations,
            n_jobs=self.n_jobs,
            scoring=self.scoringFunc
        )
        if self.printIntermediateResults is True:
            print(
                self.scoringFunc +
                &#34; Score: %0.4f (+/- %0.4f)&#34; %
                (scores.mean(), scores.std() * 2)
            )
        if self.printCoefficients is True:
            if hasattr(bestModel, &#39;coef_&#39;):
                # some models provide coefficients
                # that are set during training
                print(
                    &#34;Model&#39;s coefficients: &#34; +
                    str(bestModel.coef_)
                )
            if hasattr(bestModel, &#39;kernel_&#39;):
                # gaussian proccesses have kernel parameters
                print(
                    &#34;Kernel parameters: &#34; +
                    str(bestModel.kernel_)
                )

        endTime = datetime.datetime.now()
        runTime = endTime - startTime
        if self.printIntermediateResults is True:
            print(&#34;Duration: &#34; + str(runTime))

        return grid_model.best_score_, bestModel, bestParams

    def provideScores(
        self,
        modelBase,
        labels,
        features
    ):
        &#34;&#34;&#34;
        Does cross validation for given model in modelBase.
        Calculates MAE, MSE, RMSE, correlations and R2.
        Prints results and saves in modelBase.

        Parameters
        ----------
        modelBase : modelBase, default=None, required
            ModelBase class containing the model.
        labels : list, default=None, required
            Numeric values as prediction target.
        features : numpy.array, default=None, required
            Numeric features to predict from.
        &#34;&#34;&#34;
        # get actual estimator from object
        model = modelBase.model
        # summarize all metrics
        scoring = {
            &#39;negMAE&#39;: &#39;neg_mean_absolute_error&#39;,
            &#39;negMSE&#39;: &#39;neg_mean_squared_error&#39;,
            &#39;negRMSE&#39;: &#39;neg_root_mean_squared_error&#39;,
            &#39;R2&#39;: &#39;r2&#39;,
        }

        # cross validation to estimate scores again
        cv_results = cross_validate(
            model,
            features,
            labels,
            cv=self.crossValidationIterations,
            n_jobs=self.n_jobs,
            scoring=scoring
        )

        scores_to_save = {}

        # Printing scores for each model
        for key in scoring.keys():
            # get results, these are prefixed with test_
            key_value = (&#39;test_&#39; + str(key))
            scores = cv_results[key_value]
            values = &#34;%0.4f (+/- %0.4f)&#34; % (scores.mean(), scores.std() * 2)
            if self.printIntermediateResults is True:
                print(
                    str(key) + &#34;: &#34; +
                    str(values)
                )
            # save scores to save it later in model object
            scores_to_save[key] = values

        # save scores in model object
        modelBase.scores = scores_to_save

        return

    def startModelSelection(
        self,
        modelObjList,
        featurePipeline,
        profileColTraining,
        saveFeatures=False,
        precalculatedFeatures=None,
    ):
        &#34;&#34;&#34;
        Identify best model for each dimension.

        First, features are calculated. The model selection is done
        for each dimension (label). Labels are extracted and for each
        model in modelObjList grid search is performed.
        Then best model is identified for this dimension and added to
        globalBestModels.
        Additional scores are provided via cross validation.

        Parameters
        ----------
        modelObjList : list, default=None, required
            List of models to select from.
        featurePipeline : Pipeline, default=None, required
            Created feature pipeline to use.
        profileColTraining : ProfileCollection, default=None, required
            ProfileCollection to generate features and extract labels from.
        saveFeatures : boolean, default=False
            To save time feature calculation step can be saved and
            exported in caller function.
        precalculatedFeatures : numpy array, default=None
            If passed, those features are used and no feature calculation
            takes place.

        Returns
        -------
        globalBestModels : dict
            Dictionary with best model for each label to predict.
        &#34;&#34;&#34;
        profileList = profileColTraining.profileList

        if precalculatedFeatures is None:
            # calculate features once
            print(&#34;Calculating features&#34;)
            features = featurePipeline.fit_transform(profileList)
        else:
            print(&#34;Features from passed variable&#34;)
            features = precalculatedFeatures

        if saveFeatures is True:
            self.features = features

        print(&#34;Feature shape: &#34; + str(features.shape))
        self.features = features

        globalBestModels = {}
        scoreStatistics = {}

        # do this whole process for each label
        # (e.g. big dimensions or big 5 facets)
        for labelName in self.labelsGlobalList:
            print(
                &#34;\nModel Selection currently for label: &#34; +
                str(labelName)
            )
            # extract labels for prediction (e.g. values for Extraversion)
            labels = self.extractLabels(
                profileList=profileList,
                labelName=labelName
            )

            labelBestModels = {}

            # for each model in model list, we will perform grid search
            for modelObj in modelObjList:
                # get model
                currentModel = modelObj.getModel()
                currentGridParams = modelObj.gridSearchParams
                print(&#39;\nCurrently at model: &#39; + modelObj.modelName)
                # do crossvalidation for this model

                best_score, bestModel, bestParams = self.crossvalidateModel(
                    model=currentModel,
                    gridSearchParams=currentGridParams,
                    labels=labels,
                    features=features,
                )
                # save best model as object in dict, to select from
                # after all have been evaluated
                localModel = ModelBase(
                    labelName=labelName,
                    modelName=modelObj.modelName,
                    model=bestModel,
                    params=bestParams,
                    scores=best_score,
                    gridSearchParams=None,
                )
                labelBestModels[modelObj.modelName] = localModel
                # save score in dict, to evalute each model type
                # at the end of the function
                self._nested_set(
                    scoreStatistics,
                    [modelObj.modelName, labelName],
                    best_score
                )

            # select best model and its params by taking entry
            # with highest score
            bestModelName = max(
                labelBestModels.keys(),
                key=(lambda k: labelBestModels[k].scores)
            )
            # save it for this label in dict
            globalBestModels[labelName] = labelBestModels[bestModelName]

            # print model name
            print(
                &#34;\nBest model for &#34; +
                labelName +
                &#34; is &#34; +
                globalBestModels[labelName].modelName
            )

            # provide MAE, MSE, RMSE, correlations and R2
            # for comparison reasons
            self.provideScores(
                globalBestModels[labelName],
                labels,
                features
            )

        # for comparison identify model, that on average performs
        # best over all labels
        # meaning good scores in all dimensions for this model type
        # helps with model selection and avoid overfitting
        if self.printIntermediateResults is True:
            print(
                &#34;\nOverall average performance &#34; +
                &#34;(might help with model selection):&#34;
            )
            meanScores = {}
            for modelType in scoreStatistics:
                scores = 0
                counter = 0
                for score in scoreStatistics[modelType].values():
                    scores = scores + float(score)
                    counter = counter + 1
                mean = scores / counter
                meanScores[modelType] = mean
                print(
                    &#34;Mean score over all dimensions for model type &#34; +
                    str(modelType) +
                    &#34; is :&#34; +
                    str(mean)
                )
            # best mean score
            meanBest = max(
                meanScores.keys(),
                key=(lambda k: meanScores[k])
            )
            print(&#34;Mean best is &#34; + str(meanBest))

        return globalBestModels

    def completeModelTraining(
        self,
        modelCollection,
        featurePipeline,
        profileColTraining,
        saveFeatures=False,
        precalculatedFeatures=None,
    ):
        &#34;&#34;&#34;
        Fully train models in modelCollection.

        Features are calculated, then for each dimension model is
        fully trained and saved back in collection.

        Parameters
        ----------
        modelCollection : dict, default=None, required
            Dictionary of best models to fully train.
        featurePipeline : Pipeline, default=None, required
            Created feature pipeline to use.
        profileColTraining : ProfileCollection, default=None, required
            ProfileCollection to generate features and extract labels from.
        saveFeatures : boolean, default=False
            To save time feature calculation step can be saved and
            exported in caller function.
        precalculatedFeatures : numpy array, default=None
            If passed, those features are used and no feature calculation
            takes place.

        Returns
        -------
        modelCollection : dict
            Collection contains now fully trained models.
        &#34;&#34;&#34;
        profileList = profileColTraining.profileList

        if precalculatedFeatures is None:
            # calculate features once
            print(&#34;Calculating features for complete training&#34;)
            features = featurePipeline.fit_transform(profileList)
        else:
            print(&#34;Features from passed variable&#34;)
            features = precalculatedFeatures

        if saveFeatures is True:
            self.features = features

        for labelName in self.labelsGlobalList:
            print(
                &#34;Full Model Training currently for label: &#34; +
                str(labelName)
            )
            # extract labels for prediction (e.g. values for Extraversion)
            labels = self.extractLabels(
                profileList=profileList,
                labelName=labelName
            )

            # select the model we previously evaluated for this trait
            baseModel = modelCollection[labelName]
            model = baseModel.model

            # for each model, we will fully train the model
            model.fit(features, labels)

            # save model back in object
            baseModel.model = model
            # back into collection
            modelCollection[labelName] = baseModel

        return modelCollection

    def _nested_set(
        self,
        dic,
        keys,
        value
    ):
        &#34;&#34;&#34;
        Helps when saving scores to intialize
        dictionary.
        nested arrays
        https://stackoverflow.com/questions/13687924/
        setting-a-value-in-a-nested-python-dictionary-
        given-a-list-of-indices-and-value
        &#34;&#34;&#34;
        for key in keys[:-1]:
            dic = dic.setdefault(key, {})
        dic[keys[-1]] = value</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="miping.training.modelTraining.ModelTraining.completeModelTraining"><code class="name flex">
<span>def <span class="ident">completeModelTraining</span></span>(<span>self, modelCollection, featurePipeline, profileColTraining, saveFeatures=False, precalculatedFeatures=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fully train models in modelCollection.</p>
<p>Features are calculated, then for each dimension model is
fully trained and saved back in collection.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>modelCollection</code></strong> :&ensp;<code>dict</code>, default=<code>None, required</code></dt>
<dd>Dictionary of best models to fully train.</dd>
<dt><strong><code>featurePipeline</code></strong> :&ensp;<code>Pipeline</code>, default=<code>None, required</code></dt>
<dd>Created feature pipeline to use.</dd>
<dt><strong><code>profileColTraining</code></strong> :&ensp;<code>ProfileCollection</code>, default=<code>None, required</code></dt>
<dd>ProfileCollection to generate features and extract labels from.</dd>
<dt><strong><code>saveFeatures</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>To save time feature calculation step can be saved and
exported in caller function.</dd>
<dt><strong><code>precalculatedFeatures</code></strong> :&ensp;<code>numpy array</code>, default=<code>None</code></dt>
<dd>If passed, those features are used and no feature calculation
takes place.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>modelCollection</code></strong> :&ensp;<code>dict</code></dt>
<dd>Collection contains now fully trained models.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def completeModelTraining(
    self,
    modelCollection,
    featurePipeline,
    profileColTraining,
    saveFeatures=False,
    precalculatedFeatures=None,
):
    &#34;&#34;&#34;
    Fully train models in modelCollection.

    Features are calculated, then for each dimension model is
    fully trained and saved back in collection.

    Parameters
    ----------
    modelCollection : dict, default=None, required
        Dictionary of best models to fully train.
    featurePipeline : Pipeline, default=None, required
        Created feature pipeline to use.
    profileColTraining : ProfileCollection, default=None, required
        ProfileCollection to generate features and extract labels from.
    saveFeatures : boolean, default=False
        To save time feature calculation step can be saved and
        exported in caller function.
    precalculatedFeatures : numpy array, default=None
        If passed, those features are used and no feature calculation
        takes place.

    Returns
    -------
    modelCollection : dict
        Collection contains now fully trained models.
    &#34;&#34;&#34;
    profileList = profileColTraining.profileList

    if precalculatedFeatures is None:
        # calculate features once
        print(&#34;Calculating features for complete training&#34;)
        features = featurePipeline.fit_transform(profileList)
    else:
        print(&#34;Features from passed variable&#34;)
        features = precalculatedFeatures

    if saveFeatures is True:
        self.features = features

    for labelName in self.labelsGlobalList:
        print(
            &#34;Full Model Training currently for label: &#34; +
            str(labelName)
        )
        # extract labels for prediction (e.g. values for Extraversion)
        labels = self.extractLabels(
            profileList=profileList,
            labelName=labelName
        )

        # select the model we previously evaluated for this trait
        baseModel = modelCollection[labelName]
        model = baseModel.model

        # for each model, we will fully train the model
        model.fit(features, labels)

        # save model back in object
        baseModel.model = model
        # back into collection
        modelCollection[labelName] = baseModel

    return modelCollection</code></pre>
</details>
</dd>
<dt id="miping.training.modelTraining.ModelTraining.crossvalidateModel"><code class="name flex">
<span>def <span class="ident">crossvalidateModel</span></span>(<span>self, model, gridSearchParams, labels, features)</span>
</code></dt>
<dd>
<div class="desc"><p>Return best model from grid search and do cross validation.</p>
<p>A grid search is performed with the given parameters.
The best model selected and a cross validation with
this model performed. Scores will be printed to console.
Duration of process is printed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>modelBase.model</code>, default=<code>None, required</code></dt>
<dd>Model from sklearn to train.</dd>
<dt><strong><code>gridSearchParams</code></strong> :&ensp;<code>dict</code>, default=<code>None, required</code></dt>
<dd>Dictionary with parameters for grid search.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code>, default=<code>None, required</code></dt>
<dd>Numeric values as prediction target.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>numpy.array</code>, default=<code>None, required</code></dt>
<dd>Numeric features to predict from.</dd>
</dl>
<p>Returns
-------grid_model.best_score_, bestModel, bestParams
grid_model.best_score_ : dict
Dictionary with scores of best model.
bestModel : modelBase.model
Best performing model during gridsearch.
bestParams : dict
Parameters of best performing model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crossvalidateModel(
    self,
    model,
    gridSearchParams,
    labels,
    features
):
    &#34;&#34;&#34;
    Return best model from grid search and do cross validation.

    A grid search is performed with the given parameters.
    The best model selected and a cross validation with
    this model performed. Scores will be printed to console.
    Duration of process is printed.

    Parameters
    ----------
    model : modelBase.model, default=None, required
        Model from sklearn to train.
    gridSearchParams : dict, default=None, required
        Dictionary with parameters for grid search.
    labels : list, default=None, required
        Numeric values as prediction target.
    features : numpy.array, default=None, required
        Numeric features to predict from.

    Returns
    -------grid_model.best_score_, bestModel, bestParams
    grid_model.best_score_ : dict
        Dictionary with scores of best model.
    bestModel : modelBase.model
        Best performing model during gridsearch.
    bestParams : dict
        Parameters of best performing model.
    &#34;&#34;&#34;

    # ignore terminated early warning
    # for SVMs we set a hard limit for max iterations
    # and it would print this warning once the iterations
    # are reached.
    # somehow a bug in scikit prevents ignoring
    # when running job in parallel
    simplefilter(&#34;ignore&#34;, category=ConvergenceWarning)

    # save start time, for runtime calculation
    startTime = datetime.datetime.now()

    # set up cross validation with params
    grid_model = GridSearchCV(
        model,
        gridSearchParams,
        cv=self.crossValidationIterations,
        n_jobs=self.n_jobs,
        scoring=self.scoringFunc  # we usually use RMSE
    )

    # number of datasets available
    n = len(labels)

    # actually do grid search
    grid_model = grid_model.fit(features[:n], labels[:n])

    if self.printIntermediateResults is True:
        print(&#34;best score: &#34; + str(grid_model.best_score_))
    bestParams = {}
    for param_name in sorted(gridSearchParams.keys()):
        bestParams[param_name] = grid_model.best_params_[param_name]
        if self.printIntermediateResults is True:
            print(
                &#34;%s: %r&#34; %
                (param_name, grid_model.best_params_[param_name])
            )

    # set classifier to best classifier found by GridSearchCV
    bestModel = grid_model.best_estimator_

    # cross validation
    scores = cross_val_score(
        bestModel,
        features,
        labels,
        cv=self.crossValidationIterations,
        n_jobs=self.n_jobs,
        scoring=self.scoringFunc
    )
    if self.printIntermediateResults is True:
        print(
            self.scoringFunc +
            &#34; Score: %0.4f (+/- %0.4f)&#34; %
            (scores.mean(), scores.std() * 2)
        )
    if self.printCoefficients is True:
        if hasattr(bestModel, &#39;coef_&#39;):
            # some models provide coefficients
            # that are set during training
            print(
                &#34;Model&#39;s coefficients: &#34; +
                str(bestModel.coef_)
            )
        if hasattr(bestModel, &#39;kernel_&#39;):
            # gaussian proccesses have kernel parameters
            print(
                &#34;Kernel parameters: &#34; +
                str(bestModel.kernel_)
            )

    endTime = datetime.datetime.now()
    runTime = endTime - startTime
    if self.printIntermediateResults is True:
        print(&#34;Duration: &#34; + str(runTime))

    return grid_model.best_score_, bestModel, bestParams</code></pre>
</details>
</dd>
<dt id="miping.training.modelTraining.ModelTraining.extractLabels"><code class="name flex">
<span>def <span class="ident">extractLabels</span></span>(<span>self, profileList, labelName)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract values for one specific labelName
labels are the percentages value to predict
e.g. for Extraversion value.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>profileList</code></strong> :&ensp;<code>list</code>, default=<code>None, required</code></dt>
<dd>List of profiles to extract labels from.</dd>
<dt><strong><code>labelName</code></strong> :&ensp;<code>string</code>, default=<code>None, required</code></dt>
<dd>Label to extract from profiles.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>Numeric values used for training and prediction.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extractLabels(
    self,
    profileList,
    labelName,
):
    &#34;&#34;&#34;
    Extract values for one specific labelName
    labels are the percentages value to predict
    e.g. for Extraversion value.

    Parameters
    ----------
    profileList : list, default=None, required
        List of profiles to extract labels from.
    labelName : string, default=None, required
        Label to extract from profiles.

    Returns
    -------
    labels : list
        Numeric values used for training and prediction.
    &#34;&#34;&#34;
    # initialize return variable
    labels = []

    # loop over profile collection
    for profile in profileList:
        value = getattr(profile, labelName)
        labels.append(np.float(value))

    return labels</code></pre>
</details>
</dd>
<dt id="miping.training.modelTraining.ModelTraining.provideScores"><code class="name flex">
<span>def <span class="ident">provideScores</span></span>(<span>self, modelBase, labels, features)</span>
</code></dt>
<dd>
<div class="desc"><p>Does cross validation for given model in modelBase.
Calculates MAE, MSE, RMSE, correlations and R2.
Prints results and saves in modelBase.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>modelBase</code></strong> :&ensp;<code>modelBase</code>, default=<code>None, required</code></dt>
<dd>ModelBase class containing the model.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code>, default=<code>None, required</code></dt>
<dd>Numeric values as prediction target.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>numpy.array</code>, default=<code>None, required</code></dt>
<dd>Numeric features to predict from.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def provideScores(
    self,
    modelBase,
    labels,
    features
):
    &#34;&#34;&#34;
    Does cross validation for given model in modelBase.
    Calculates MAE, MSE, RMSE, correlations and R2.
    Prints results and saves in modelBase.

    Parameters
    ----------
    modelBase : modelBase, default=None, required
        ModelBase class containing the model.
    labels : list, default=None, required
        Numeric values as prediction target.
    features : numpy.array, default=None, required
        Numeric features to predict from.
    &#34;&#34;&#34;
    # get actual estimator from object
    model = modelBase.model
    # summarize all metrics
    scoring = {
        &#39;negMAE&#39;: &#39;neg_mean_absolute_error&#39;,
        &#39;negMSE&#39;: &#39;neg_mean_squared_error&#39;,
        &#39;negRMSE&#39;: &#39;neg_root_mean_squared_error&#39;,
        &#39;R2&#39;: &#39;r2&#39;,
    }

    # cross validation to estimate scores again
    cv_results = cross_validate(
        model,
        features,
        labels,
        cv=self.crossValidationIterations,
        n_jobs=self.n_jobs,
        scoring=scoring
    )

    scores_to_save = {}

    # Printing scores for each model
    for key in scoring.keys():
        # get results, these are prefixed with test_
        key_value = (&#39;test_&#39; + str(key))
        scores = cv_results[key_value]
        values = &#34;%0.4f (+/- %0.4f)&#34; % (scores.mean(), scores.std() * 2)
        if self.printIntermediateResults is True:
            print(
                str(key) + &#34;: &#34; +
                str(values)
            )
        # save scores to save it later in model object
        scores_to_save[key] = values

    # save scores in model object
    modelBase.scores = scores_to_save

    return</code></pre>
</details>
</dd>
<dt id="miping.training.modelTraining.ModelTraining.startModelSelection"><code class="name flex">
<span>def <span class="ident">startModelSelection</span></span>(<span>self, modelObjList, featurePipeline, profileColTraining, saveFeatures=False, precalculatedFeatures=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Identify best model for each dimension.</p>
<p>First, features are calculated. The model selection is done
for each dimension (label). Labels are extracted and for each
model in modelObjList grid search is performed.
Then best model is identified for this dimension and added to
globalBestModels.
Additional scores are provided via cross validation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>modelObjList</code></strong> :&ensp;<code>list</code>, default=<code>None, required</code></dt>
<dd>List of models to select from.</dd>
<dt><strong><code>featurePipeline</code></strong> :&ensp;<code>Pipeline</code>, default=<code>None, required</code></dt>
<dd>Created feature pipeline to use.</dd>
<dt><strong><code>profileColTraining</code></strong> :&ensp;<code>ProfileCollection</code>, default=<code>None, required</code></dt>
<dd>ProfileCollection to generate features and extract labels from.</dd>
<dt><strong><code>saveFeatures</code></strong> :&ensp;<code>boolean</code>, default=<code>False</code></dt>
<dd>To save time feature calculation step can be saved and
exported in caller function.</dd>
<dt><strong><code>precalculatedFeatures</code></strong> :&ensp;<code>numpy array</code>, default=<code>None</code></dt>
<dd>If passed, those features are used and no feature calculation
takes place.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>globalBestModels</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary with best model for each label to predict.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def startModelSelection(
    self,
    modelObjList,
    featurePipeline,
    profileColTraining,
    saveFeatures=False,
    precalculatedFeatures=None,
):
    &#34;&#34;&#34;
    Identify best model for each dimension.

    First, features are calculated. The model selection is done
    for each dimension (label). Labels are extracted and for each
    model in modelObjList grid search is performed.
    Then best model is identified for this dimension and added to
    globalBestModels.
    Additional scores are provided via cross validation.

    Parameters
    ----------
    modelObjList : list, default=None, required
        List of models to select from.
    featurePipeline : Pipeline, default=None, required
        Created feature pipeline to use.
    profileColTraining : ProfileCollection, default=None, required
        ProfileCollection to generate features and extract labels from.
    saveFeatures : boolean, default=False
        To save time feature calculation step can be saved and
        exported in caller function.
    precalculatedFeatures : numpy array, default=None
        If passed, those features are used and no feature calculation
        takes place.

    Returns
    -------
    globalBestModels : dict
        Dictionary with best model for each label to predict.
    &#34;&#34;&#34;
    profileList = profileColTraining.profileList

    if precalculatedFeatures is None:
        # calculate features once
        print(&#34;Calculating features&#34;)
        features = featurePipeline.fit_transform(profileList)
    else:
        print(&#34;Features from passed variable&#34;)
        features = precalculatedFeatures

    if saveFeatures is True:
        self.features = features

    print(&#34;Feature shape: &#34; + str(features.shape))
    self.features = features

    globalBestModels = {}
    scoreStatistics = {}

    # do this whole process for each label
    # (e.g. big dimensions or big 5 facets)
    for labelName in self.labelsGlobalList:
        print(
            &#34;\nModel Selection currently for label: &#34; +
            str(labelName)
        )
        # extract labels for prediction (e.g. values for Extraversion)
        labels = self.extractLabels(
            profileList=profileList,
            labelName=labelName
        )

        labelBestModels = {}

        # for each model in model list, we will perform grid search
        for modelObj in modelObjList:
            # get model
            currentModel = modelObj.getModel()
            currentGridParams = modelObj.gridSearchParams
            print(&#39;\nCurrently at model: &#39; + modelObj.modelName)
            # do crossvalidation for this model

            best_score, bestModel, bestParams = self.crossvalidateModel(
                model=currentModel,
                gridSearchParams=currentGridParams,
                labels=labels,
                features=features,
            )
            # save best model as object in dict, to select from
            # after all have been evaluated
            localModel = ModelBase(
                labelName=labelName,
                modelName=modelObj.modelName,
                model=bestModel,
                params=bestParams,
                scores=best_score,
                gridSearchParams=None,
            )
            labelBestModels[modelObj.modelName] = localModel
            # save score in dict, to evalute each model type
            # at the end of the function
            self._nested_set(
                scoreStatistics,
                [modelObj.modelName, labelName],
                best_score
            )

        # select best model and its params by taking entry
        # with highest score
        bestModelName = max(
            labelBestModels.keys(),
            key=(lambda k: labelBestModels[k].scores)
        )
        # save it for this label in dict
        globalBestModels[labelName] = labelBestModels[bestModelName]

        # print model name
        print(
            &#34;\nBest model for &#34; +
            labelName +
            &#34; is &#34; +
            globalBestModels[labelName].modelName
        )

        # provide MAE, MSE, RMSE, correlations and R2
        # for comparison reasons
        self.provideScores(
            globalBestModels[labelName],
            labels,
            features
        )

    # for comparison identify model, that on average performs
    # best over all labels
    # meaning good scores in all dimensions for this model type
    # helps with model selection and avoid overfitting
    if self.printIntermediateResults is True:
        print(
            &#34;\nOverall average performance &#34; +
            &#34;(might help with model selection):&#34;
        )
        meanScores = {}
        for modelType in scoreStatistics:
            scores = 0
            counter = 0
            for score in scoreStatistics[modelType].values():
                scores = scores + float(score)
                counter = counter + 1
            mean = scores / counter
            meanScores[modelType] = mean
            print(
                &#34;Mean score over all dimensions for model type &#34; +
                str(modelType) +
                &#34; is :&#34; +
                str(mean)
            )
        # best mean score
        meanBest = max(
            meanScores.keys(),
            key=(lambda k: meanScores[k])
        )
        print(&#34;Mean best is &#34; + str(meanBest))

    return globalBestModels</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="miping.training" href="index.html">miping.training</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="miping.training.modelTraining.ModelTraining" href="#miping.training.modelTraining.ModelTraining">ModelTraining</a></code></h4>
<ul class="">
<li><code><a title="miping.training.modelTraining.ModelTraining.completeModelTraining" href="#miping.training.modelTraining.ModelTraining.completeModelTraining">completeModelTraining</a></code></li>
<li><code><a title="miping.training.modelTraining.ModelTraining.crossvalidateModel" href="#miping.training.modelTraining.ModelTraining.crossvalidateModel">crossvalidateModel</a></code></li>
<li><code><a title="miping.training.modelTraining.ModelTraining.extractLabels" href="#miping.training.modelTraining.ModelTraining.extractLabels">extractLabels</a></code></li>
<li><code><a title="miping.training.modelTraining.ModelTraining.provideScores" href="#miping.training.modelTraining.ModelTraining.provideScores">provideScores</a></code></li>
<li><code><a title="miping.training.modelTraining.ModelTraining.startModelSelection" href="#miping.training.modelTraining.ModelTraining.startModelSelection">startModelSelection</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>